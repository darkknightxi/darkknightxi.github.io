{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Building Stable Diffusion from its Components\n",
    "author: Kartikeya Khare\n",
    "date: 'January 31st, 2024'\n",
    "image: philosopher dog.png\n",
    "categories:\n",
    "  - Deep Learning\n",
    "  - Generative AI\n",
    "  - Stable Diffusion\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-location: left\n",
    "    toc-title: Contents\n",
    "execute:\n",
    "  enabled: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A code-first introduction to Stable Diffusion using the Diffusers library.\n",
    "\n",
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq diffusers transformers fastcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from fastcore.all import concat\n",
    "from huggingface_hub import notebook_login\n",
    "from PIL import Image\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "if not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    w,h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion\n",
    "\n",
    "Before diving into the code, let's first take a bird's eye view of how Stable Diffusion works. Stable Diffusion is a latent diffusion algorithm, meaning it works with the images in latent space, that is compresses the images into a smaller dimesnion using an autoencoder. This makes the process both faster and memory efficient. In the forward process of diffusion, Gaussian noise is added to the images using an encoder over a number of steps till the images is indistinguishable from white noise. In the backward process the images are iteratively denoised to generate new samples. The denoising is done using a unet model that predicts and subtracts noise which is then subtracted from the image.\n",
    "\n",
    "![](Diffusion.png)\n",
    "\n",
    "# The Diffusion Pipeline\n",
    "\n",
    "Diffusers library by Hugging Face is arguably the best way to use Stable Diffusion. The Diffusers library uses `StableDiffusionPipeline`, which is an end-to-end diffusion inference pipeline to combine all the steps in the diffusion process to easily do inference with the model. However, in this tutorial we will not use the pipeline directly ourselves but rather build it ourselves from its components to get a better understanding of stable diffusion works under the hood. This also gives us more control over the output generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "089f7b348b084443846d6a0b3470f571",
      "d8b47c0e01524fc788b216a640f95b2b",
      "b58caef5e3464d16a2f9895a30248fc2",
      "f065144ca5804bb3a0d44ec613f82abf",
      "f59b2ab8269a4dd399bef190dddf8cbb",
      "dc79446c1afd421c89f0be6d08857ea7",
      "61db1e2ca24740148f18dc465441f173",
      "d9ca77e25e03479b8037446453f1ca23",
      "888531cc9dea4de5bbed6bc43fe0ff35",
      "d424bf325bf84e93945ec157cc70669f",
      "0600d9594079467abee6cf072af53219"
     ]
    }
   },
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", variant=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking inside the pipeline\n",
    "\n",
    "The diffusion pipeline combines all the components of stable diffusion for convenient inference. Let's build the pipeline back by putting together its components.\n",
    "\n",
    "The main componets of a latent diffusion algorithm are:\n",
    "\n",
    "1. An autoencoder (VAE)- for compressing the image in the latent space\n",
    "2. A u-net- for predicting noise in the latent image\n",
    "3. A text encoder: for creating embeddings of text and images.  \n",
    "\n",
    "The output of a u-net, the noise residual, is used for gradual denoising of the latent image representation via a scheduler algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need a text tokenizer and text encoder. Stable Diffusion uses the CLIP encoder by Open AI, so we'll be using its weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the `vae` and `unet` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, UNet2DConditionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use a different VAE to the original release, which has been fine-tuned for more steps\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard pipeline used the PNDM scheduler, but we'll use the K-LMS scheduler.\n",
    "We need to use the same noising schedule as was used during training. The schedule is defined by the time step at which the noise is added and the amount of noise that is added, which is given by the beta parameters.\n",
    "For the K-LMS scheduler, this is how the betas evolve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    }
   },
   "outputs": [],
   "source": [
    "beta_start, beta_end = 0.00085, 0.012\n",
    "plt.plot(torch.linspace(beta_start ** 0.5, beta_end ** 0.5, 1000) ** 2)\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Î²')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule='scaled_linear', num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now define the parameters we'll use for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Batman drinking coffee while sitting in a cafe\"\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 70\n",
    "guidance_scale = 7.5\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tokenize the prompt. To ensure that we have same number of tokens for each prompt, we'll use max length padding and truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "text_input = tokenizer(prompt, padding='max_length', max_length=tokenizer.model_max_length, truncation=True, return_tensors='pt')\n",
    "text_input['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(49407), tokenizer.decode(49406)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "text_input['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text encoder gives us the embeddings for the text prompt we have used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "text_embeddings = text_encoder(text_input.input_ids.to('cuda'))[0].half()\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also get the embeddings required for unconditional generation, which is achieved with an empty string; the model is free to go anywhere as long as the results as reasonable. These embeddings will be used for classifier-free guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer(['']*batch_size, padding='max_length',max_length=max_length, return_tensors='pt')\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to('cuda'))[0].half()\n",
    "uncond_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classifier-free guidance, we need to do two forward passes. One with the conditioned input (`text_embeddings`), and another with the unconditional embeddings (`uncond_embeddings`). In practice, we can concatenate both into a single batch to avoid doing two forward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "text_embeddings = torch.cat([text_embeddings, uncond_embeddings])\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the denoising process, we'll start from pure Gaussian noise. These are our initial latents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "latents = torch.randn((batch_size, unet.in_channels, height//8, width//8))\n",
    "latents = latents.to('cuda').half()\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the scheduler with `num_inference_steps`. This will prepare the internal state to be used during denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(num_inference_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the latents with the initial standard deviation required by the scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = latents * scheduler.init_noise_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to write the denoising loop. The timesteps go from `999` to `0` following a given schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "scheduler.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "scheduler.sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(scheduler.timesteps, scheduler.sigmas[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "5d075a9161d54af88a2853701db1cf90",
      "34a4f0dffba94b7891885cea42c5af20",
      "d927febf2f2341aba2affa8595a03565",
      "0b1bede609784d909f0b557f178d8b49",
      "7f944e20224948c1977efb625c93eb05",
      "5e67ad32f02547db8d044d2a09c4f9f9",
      "91258726f98145bbabd401a415f37c06",
      "e02d84fed260473ebb5f37d24017598a",
      "34df027cec0143d085d89e68d061a84f",
      "1a20092da5c543c8975d33cc10935d4d",
      "8f97abe7c04e4a5198a9cfa0452284e5"
     ]
    }
   },
   "outputs": [],
   "source": [
    "for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
    "    input = torch.cat([latents]*2)\n",
    "    input = scheduler.scale_model_input(input, t)\n",
    "\n",
    "    #predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    #perform guidance\n",
    "    pred_text, pred_uncond = pred.chunk(2)\n",
    "    pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n",
    "\n",
    "    #compute the previous 'noisy' sample\n",
    "    latents = scheduler.step(pred, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our latents contain the denoised representation of the image. We use the `vae` decoder to convert it back to the pixel-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): image = vae.decode(1 / 0.18215 * latents).sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, lets convert it back to `PIL` to display the image.\n",
    "\n",
    "The arithmetic acrobats are done to convert the decoded image in a form that can be used correctly by the `PIL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    }
   },
   "outputs": [],
   "source": [
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image[0].detach().cpu().permute(1, 2, 0).numpy()\n",
    "image = (image * 255).round().astype(\"uint8\")\n",
    "Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! Here's our Batman with his coffee.\n",
    "\n",
    "This was the text2image pipeline. Now, let's build the img2img pipeline. Instead of starting with pure noise, we'll start with a certain image and add noise to it. We skip the initial steps of the denoising process and pretend that the given image is what the algorithm came up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "!curl --output philosophical_dog.jpg 'https://puppytoob.com/wp-content/uploads/2013/04/Dogs_In_Wind_8.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    }
   },
   "outputs": [],
   "source": [
    "input_image = Image.open('philosophical_dog.jpg').resize((512, 512))\n",
    "input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to encode our image into the latent space using `vae`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_latent(input_im):\n",
    "    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to('cuda').half()*2 - 1)\n",
    "    return 0.18215 * latent.latent_dist.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a function to get the image back from latent representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latents_to_pil(latents):\n",
    "    # batch of latents -> list of images\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "encoded = pil_to_latent(input_image)\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's repeat the process we did above, except that we will start with the input image and not with pure noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561,
     "referenced_widgets": [
      "f3008e185c0346dd94f49907b0d33400",
      "7076a867c85a4706b273bb2b571a8e2a",
      "7856e871755b4e0eaf16632930cfc195",
      "6f37cb02c7c24354bdb80df8546dfbec",
      "adc7dc03e18b44b48bc39c173dd4a518",
      "49241926afd14020a0699b0f6bbe04eb",
      "c9e81330f0d8473ea134a27ad645c4c3",
      "0ba4ff42a48c45028903d512bc48de73",
      "d1a8b96143e5456d904e88cb5a82e472",
      "1668a5fa51bc4102a7a301936f9f8910",
      "d7ef23877bb747a997f65c941b4b5a75"
     ]
    }
   },
   "outputs": [],
   "source": [
    "#set parameters\n",
    "prompt = ['A painting in the style of Vincent Van Gogh']\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 70\n",
    "guidance_scale = 7.5\n",
    "batch_size = 1\n",
    "\n",
    "# Prep text embeddings\n",
    "text_input = tokenizer(prompt, padding='max_length', max_length=tokenizer.model_max_length, truncation=True, return_tensors='pt')\n",
    "text_embeddings = text_encoder(text_input.input_ids.to('cuda'))[0].half()\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer([\"\"]*batch_size, padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to('cuda'))[0].half()\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "#Prep scheduler (set num of inference steps)\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "#Prep latents\n",
    "start_step = 30\n",
    "start_sigma = scheduler.sigmas[start_step]\n",
    "noise = torch.randn_like(encoded)\n",
    "latents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n",
    "latents = latents.to('cuda').half()\n",
    "\n",
    "#Denoising loop\n",
    "for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
    "    if i > start_step:\n",
    "        input = torch.concat([latents]*2)\n",
    "        input = scheduler.scale_model_input(input, t)\n",
    "\n",
    "        #predict noise residual\n",
    "        with torch.no_grad():\n",
    "            pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        #perform guidance\n",
    "        pred_uncond, pred_text = pred.chunk(2)\n",
    "        pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n",
    "\n",
    "        #compute prev noisy sample\n",
    "        latents = scheduler.step(pred, t, latents).prev_sample\n",
    "\n",
    "latents_to_pil(latents)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's our dog, ruminating about the meaninglessness of life.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
