---
title: Building Stable Diffusion from its Components
author: Kartikeya Khare
date: 'January 31st, 2024'
image: philosopher dog.png
categories:
  - Deep Learning
  - Generative AI
  - Stable Diffusion
format:
  html:
    toc: true
    toc-location: left
    toc-title: Contents
execute:
  enabled: true
jupyter: python3
---

A code-first introduction to Stable Diffusion using the Diffusers library.

# Importing Libraries

```{python}
!pip install -Uq diffusers transformers fastcore
```

```{python}
import logging
from pathlib import Path

import matplotlib.pyplot as plt
import torch
from diffusers import StableDiffusionPipeline
from fastcore.all import concat
from huggingface_hub import notebook_login
from PIL import Image

logging.disable(logging.WARNING)

torch.manual_seed(1)
if not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()
```

```{python}
def image_grid(imgs, rows, cols):
    w,h = imgs[0].size
    grid = Image.new('RGB', size=(cols*w, rows*h))
    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))
    return grid
```

# Stable Diffusion

Before diving into the code, let's first take a bird's eye view of how Stable Diffusion works. Stable Diffusion is a latent diffusion algorithm, meaning it works with the images in latent space, that is compresses the images into a smaller dimesnion using an autoencoder. This makes the process both faster and memory efficient. In the forward process of diffusion, Gaussian noise is added to the images using an encoder over a number of steps till the images is indistinguishable from white noise. In the backward process the images are iteratively denoised to generate new samples. The denoising is done using a unet model that predicts and subtracts noise which is then subtracted from the image.

![](Diffusion.png)

# The Diffusion Pipeline

Diffusers library by Hugging Face is arguably the best way to use Stable Diffusion. The Diffusers library uses `StableDiffusionPipeline`, which is an end-to-end diffusion inference pipeline to combine all the steps in the diffusion process to easily do inference with the model. However, in this tutorial we will not use the pipeline directly ourselves but rather build it ourselves from its components to get a better understanding of stable diffusion works under the hood. This also gives us more control over the output generated by the model.

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 173, referenced_widgets: [089f7b348b084443846d6a0b3470f571, d8b47c0e01524fc788b216a640f95b2b, b58caef5e3464d16a2f9895a30248fc2, f065144ca5804bb3a0d44ec613f82abf, f59b2ab8269a4dd399bef190dddf8cbb, dc79446c1afd421c89f0be6d08857ea7, 61db1e2ca24740148f18dc465441f173, d9ca77e25e03479b8037446453f1ca23, 888531cc9dea4de5bbed6bc43fe0ff35, d424bf325bf84e93945ec157cc70669f, 0600d9594079467abee6cf072af53219]}
pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", variant="fp16", torch_dtype=torch.float16).to("cuda")
```

# Looking inside the pipeline

The diffusion pipeline combines all the components of stable diffusion for convenient inference. Let's build the pipeline back by putting together its components.

The main componets of a latent diffusion algorithm are:

1. An autoencoder (VAE)- for compressing the image in the latent space
2. A u-net- for predicting noise in the latent image
3. A text encoder: for creating embeddings of text and images.  

The output of a u-net, the noise residual, is used for gradual denoising of the latent image representation via a scheduler algorithm.

```{python}
del pipe
```

First, we need a text tokenizer and text encoder. Stable Diffusion uses the CLIP encoder by Open AI, so we'll be using its weights:

```{python}
from transformers import CLIPTextModel, CLIPTokenizer
```

```{python}
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16)
text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")
```

Next, we'll use the `vae` and `unet` models.

```{python}
from diffusers import AutoencoderKL, UNet2DConditionModel
```

```{python}
# Here we use a different VAE to the original release, which has been fine-tuned for more steps
vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-ema", torch_dtype=torch.float16).to("cuda")
unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")
```

The standard pipeline used the PNDM scheduler, but we'll use the K-LMS scheduler.
We need to use the same noising schedule as was used during training. The schedule is defined by the time step at which the noise is added and the amount of noise that is added, which is given by the beta parameters.
For the K-LMS scheduler, this is how the betas evolve:

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 466}
beta_start, beta_end = 0.00085, 0.012
plt.plot(torch.linspace(beta_start ** 0.5, beta_end ** 0.5, 1000) ** 2)
plt.xlabel('Timestep')
plt.ylabel('Î²')
```

```{python}
from diffusers import LMSDiscreteScheduler
```

```{python}
scheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule='scaled_linear', num_train_timesteps=1000)
```

We'll now define the parameters we'll use for generation

```{python}
prompt = "Batman drinking coffee while sitting in a cafe"

height = 512
width = 512
num_inference_steps = 70
guidance_scale = 7.5
batch_size = 1
```

Next, we tokenize the prompt. To ensure that we have same number of tokens for each prompt, we'll use max length padding and truncation.

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
text_input = tokenizer(prompt, padding='max_length', max_length=tokenizer.model_max_length, truncation=True, return_tensors='pt')
text_input['input_ids']
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
tokenizer.decode(49407), tokenizer.decode(49406)
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
text_input['attention_mask']
```

The text encoder gives us the embeddings for the text prompt we have used.

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
text_embeddings = text_encoder(text_input.input_ids.to('cuda'))[0].half()
text_embeddings.shape
```

We'll also get the embeddings required for unconditional generation, which is achieved with an empty string; the model is free to go anywhere as long as the results as reasonable. These embeddings will be used for classifier-free guidance.

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
max_length = text_input.input_ids.shape[-1]
uncond_input = tokenizer(['']*batch_size, padding='max_length',max_length=max_length, return_tensors='pt')
uncond_embeddings = text_encoder(uncond_input.input_ids.to('cuda'))[0].half()
uncond_embeddings.shape
```

For classifier-free guidance, we need to do two forward passes. One with the conditioned input (`text_embeddings`), and another with the unconditional embeddings (`uncond_embeddings`). In practice, we can concatenate both into a single batch to avoid doing two forward passes.

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
text_embeddings = torch.cat([text_embeddings, uncond_embeddings])
text_embeddings.shape
```

To start the denoising process, we'll start from pure Gaussian noise. These are our initial latents.

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
torch.manual_seed(42)
latents = torch.randn((batch_size, unet.in_channels, height//8, width//8))
latents = latents.to('cuda').half()
latents.shape
```

We initialize the scheduler with `num_inference_steps`. This will prepare the internal state to be used during denoising.

```{python}
scheduler.set_timesteps(num_inference_steps)
```

We scale the latents with the initial standard deviation required by the scheduler.

```{python}
latents = latents * scheduler.init_noise_sigma
```

We are now ready to write the denoising loop. The timesteps go from `999` to `0` following a given schedule.

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
scheduler.timesteps
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
scheduler.sigmas
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 447}
plt.plot(scheduler.timesteps, scheduler.sigmas[:-1])
```

```{python}
from tqdm.auto import tqdm
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 49, referenced_widgets: [5d075a9161d54af88a2853701db1cf90, 34a4f0dffba94b7891885cea42c5af20, d927febf2f2341aba2affa8595a03565, 0b1bede609784d909f0b557f178d8b49, 7f944e20224948c1977efb625c93eb05, 5e67ad32f02547db8d044d2a09c4f9f9, 91258726f98145bbabd401a415f37c06, e02d84fed260473ebb5f37d24017598a, 34df027cec0143d085d89e68d061a84f, 1a20092da5c543c8975d33cc10935d4d, 8f97abe7c04e4a5198a9cfa0452284e5]}
for i, t in enumerate(tqdm(scheduler.timesteps)):
    input = torch.cat([latents]*2)
    input = scheduler.scale_model_input(input, t)

    #predict the noise residual
    with torch.no_grad():
        pred = unet(input, t, encoder_hidden_states=text_embeddings).sample

    #perform guidance
    pred_text, pred_uncond = pred.chunk(2)
    pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)

    #compute the previous 'noisy' sample
    latents = scheduler.step(pred, t, latents).prev_sample
```

Now, our latents contain the denoised representation of the image. We use the `vae` decoder to convert it back to the pixel-space.

```{python}
with torch.no_grad(): image = vae.decode(1 / 0.18215 * latents).sample
```

And finally, lets convert it back to `PIL` to display the image.

The arithmetic acrobats are done to convert the decoded image in a form that can be used correctly by the `PIL`.

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 529}
image = (image / 2 + 0.5).clamp(0, 1)
image = image[0].detach().cpu().permute(1, 2, 0).numpy()
image = (image * 255).round().astype("uint8")
Image.fromarray(image)
```

Voila! Here's our Batman with his coffee.

This was the text2image pipeline. Now, let's build the img2img pipeline. Instead of starting with pure noise, we'll start with a certain image and add noise to it. We skip the initial steps of the denoising process and pretend that the given image is what the algorithm came up with.

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
!curl --output philosophical_dog.jpg 'https://puppytoob.com/wp-content/uploads/2013/04/Dogs_In_Wind_8.jpg'
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 529}
input_image = Image.open('philosophical_dog.jpg').resize((512, 512))
input_image
```

Let's create a function to encode our image into the latent space using `vae`.

```{python}
from torchvision import transforms as tfms
```

```{python}
def pil_to_latent(input_im):
    # Single image -> single latent in a batch (so size 1, 4, 64, 64)
    with torch.no_grad():
        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to('cuda').half()*2 - 1)
    return 0.18215 * latent.latent_dist.sample()
```

... and a function to get the image back from latent representation

```{python}
def latents_to_pil(latents):
    # batch of latents -> list of images
    latents = (1 / 0.18215) * latents
    with torch.no_grad():
        image = vae.decode(latents).sample
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()
    images = (image * 255).round().astype("uint8")
    pil_images = [Image.fromarray(image) for image in images]
    return pil_images
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
encoded = pil_to_latent(input_image)
encoded.shape
```

Now, let's repeat the process we did above, except that we will start with the input image and not with pure noise.

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 561, referenced_widgets: [f3008e185c0346dd94f49907b0d33400, 7076a867c85a4706b273bb2b571a8e2a, 7856e871755b4e0eaf16632930cfc195, 6f37cb02c7c24354bdb80df8546dfbec, adc7dc03e18b44b48bc39c173dd4a518, 49241926afd14020a0699b0f6bbe04eb, c9e81330f0d8473ea134a27ad645c4c3, 0ba4ff42a48c45028903d512bc48de73, d1a8b96143e5456d904e88cb5a82e472, 1668a5fa51bc4102a7a301936f9f8910, d7ef23877bb747a997f65c941b4b5a75]}
#set parameters
prompt = ['A painting in the style of Vincent Van Gogh']
height = 512
width = 512
num_inference_steps = 70
guidance_scale = 7.5
batch_size = 1

# Prep text embeddings
text_input = tokenizer(prompt, padding='max_length', max_length=tokenizer.model_max_length, truncation=True, return_tensors='pt')
text_embeddings = text_encoder(text_input.input_ids.to('cuda'))[0].half()
max_length = text_input.input_ids.shape[-1]
uncond_input = tokenizer([""]*batch_size, padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')
uncond_embeddings = text_encoder(uncond_input.input_ids.to('cuda'))[0].half()
text_embeddings = torch.cat([uncond_embeddings, text_embeddings])

#Prep scheduler (set num of inference steps)
scheduler.set_timesteps(num_inference_steps)

#Prep latents
start_step = 30
start_sigma = scheduler.sigmas[start_step]
noise = torch.randn_like(encoded)
latents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))
latents = latents.to('cuda').half()

#Denoising loop
for i, t in enumerate(tqdm(scheduler.timesteps)):
    if i > start_step:
        input = torch.concat([latents]*2)
        input = scheduler.scale_model_input(input, t)

        #predict noise residual
        with torch.no_grad():
            pred = unet(input, t, encoder_hidden_states=text_embeddings).sample

        #perform guidance
        pred_uncond, pred_text = pred.chunk(2)
        pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)

        #compute prev noisy sample
        latents = scheduler.step(pred, t, latents).prev_sample

latents_to_pil(latents)[0]
```

And here's our dog, ruminating about the meaninglessness of life.

