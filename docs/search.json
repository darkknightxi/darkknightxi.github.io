[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kartikeya’s Musings",
    "section": "",
    "text": "Writing about my adventures as I dive deep into the world of Deep Learning, Literature, Movies, Music, and Life. You will find me writing technical blogs on deep learning, sharing what I learn, and occasionally reflecting on things as I experience them."
  },
  {
    "objectID": "blog/about.html",
    "href": "blog/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog/posts/post-with-code/index.html",
    "href": "blog/posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/welcome/index.html",
    "href": "blog/posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Neural Style Transfer\n\n\n\n\n\n\nDeep Learning\n\n\nGenerative AI\n\n\n\n\n\n\n\n\n\nMar 18, 2024\n\n\nKartikeya Khare\n\n\n\n\n\n\n\n\n\n\n\n\nDenoising Diffusion Probabilistic Models\n\n\n\n\n\n\nDeep Learning\n\n\nGenerative AI\n\n\nDiffusion\n\n\n\n\n\n\n\n\n\nMar 14, 2024\n\n\nKartikeya Khare\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Neural Networks from Scratch\n\n\n\n\n\n\nDeep Learning\n\n\nComputer Vision\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nKartikeya Khare\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Stable Diffusion from its Components\n\n\n\n\n\n\nDeep Learning\n\n\nGenerative AI\n\n\nDiffusion\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nKartikeya Khare\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/ConvNets/ConvNets.html",
    "href": "blog/posts/ConvNets/ConvNets.html",
    "title": "Convolutional Neural Networks from Scratch",
    "section": "",
    "text": "Building our own Convolutional Neural Network from scratch.\nThis blog post is a code-first introduction to Convolutional Networks (ConvNets). We will build our own ConvNet from scratch and understand the fundamentals of how and why they work so well, particularly for image processing tasks.\nNB: This blog uses some functions from the little_ai library I am currently building.\n\nImporting Libraries\n\nimport torch\nfrom torch import nn\n\nfrom torch.utils.data import default_collate\nfrom typing import Mapping\n\nfrom little_ai.training import *\nfrom little_ai.datasets import *\n\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\nimport pandas as pd,matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\nfrom torch import tensor\nimport torch.nn.functional as F\nfrom torch import optim\n\nfrom torch.utils.data import DataLoader\nfrom typing import Mapping\n\n\nmpl.rcParams['image.cmap'] = 'gray'\n\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n\n\n\nWhy ConvNets\n &gt; Figure 1: A simple neural net with two hidden layers\nImages have properties that necessitate the need for specialized architectures.\n\nThey are high-dimensional. RGB images of dimensions \\(224\\times224\\) are very common (i.e., \\(150, 528\\) input dimensions). Hidden layers in fully connected networks are generally larger than the input size, so even for a shallow network, the number of weights would exceed \\(150,528^2\\) or \\(22\\) billion. This poses obvious practical problems in terms of the required training data, memory, and computation.\nNearby image pixels are statistically related. However, fully connected networks have no notion of “nearby” and treat the relation between every input equally.\nThe interpretation of an image is stable under geometric transformations. An image of a tree is still an image of a tree if we shift it leftwards by a few pixels. However, this shift is stable under geometric transformations. Hence, a fully connected model must learn the patterns of pixels that signify a tree separately at every position, which is inefficient.\n\nConvolutional layers process each local image region independently, using parameters shared across the whole image. They use fewer parameters than fully connected layers, exploit the spatial relationships between nearby pixels, and don’t have to re-learn the interpretation of pixels at every position.\n &gt; Figure 2: Architecture of a simple ConvNet\n\n\nConvolution Operation\nConvolutional layers perform the convolution operation using kernels (or filters) which is simply a matrix, over an image. Convolution is a simple mathematical operation that involves addition and multiplication.\n\n\n\nConvolution Operation\n\n\n\nFigure 3: Convolution operation using a \\(2\\times2\\) kernel over a \\(3\\times4\\) image that results in a \\(2\\times3\\) output matrix. The kernel slides over the image, adding the products of the overlapping matrix values. Source: Deep Learning by Goodfellow, et al.\n\nNow, let’s implement our own convolution operation using numpy. We’ll choose an image from the training set and create a simple \\(3\\times 3\\) matrix as our kernel and, as we’ll see, the convolution operation with the right choice of the kernel will be able to identify some pattern in our image.\n\nx_imgs = x_train.view(-1, 28, 28)\nxv_imgs = x_valid.view(-1, 28, 28)\n\n\nmpl.rcParams['figure.dpi'] = 30\n\n\nim3 = x_imgs[7]\nshow_image(im3);\n\n\n\n\n\n\n\n\nLet’s create a simple \\(3\\times3\\) kernel (people in the computer vision world call it a kernel; essentially it’s just a tensor).\nNB: Actually, the values of kernels are not entered manually but are learned as parameters during training.\n\ntop_edge = tensor([[-1, -1, -1], \n                  [0, 0, 0], \n                  [1, 1, 1]]).float()\n\n\nshow_image(top_edge, noframe=False);\n\n\n\n\n\n\n\n\nThis kernel will slide along all the \\(3\\times 3\\) windows in our images and compute the convolutions. If \\(3\\times 3\\)` window looks like this:\n\\[\\begin{matrix} a & b & c \\\\ d & e & f \\\\ g & h & i\\end{matrix}\\]\nthen the result will be \\(-a -b -c +g+h+i\\)\n\ndf = pd.DataFrame(im3[:13,:23])\ndf.style.format(precision=2).set_properties(**{'font-size':'7pt'}).background_gradient('Greys')\n\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n\n0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n2\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n3\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n4\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n5\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.15\n0.17\n0.41\n1.00\n0.99\n0.99\n0.99\n0.99\n0.99\n0.68\n0.02\n0.00\n\n\n6\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.17\n0.54\n0.88\n0.88\n0.98\n0.99\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.62\n0.05\n\n\n7\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.70\n0.98\n0.98\n0.98\n0.98\n0.99\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.23\n\n\n8\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.43\n0.98\n0.98\n0.90\n0.52\n0.52\n0.52\n0.52\n0.74\n0.98\n0.98\n0.98\n0.98\n0.23\n\n\n9\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.11\n0.11\n0.09\n0.00\n0.00\n0.00\n0.00\n0.05\n0.88\n0.98\n0.98\n0.67\n0.03\n\n\n10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\n0.95\n0.98\n0.98\n0.56\n0.00\n\n\n11\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.34\n0.74\n0.98\n0.98\n0.98\n0.05\n0.00\n\n\n12\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.36\n0.83\n0.96\n0.98\n0.98\n0.98\n0.80\n0.04\n0.00\n\n\n\n\n\n\nIf we compute the convolution over the \\(3\\times 3\\) window along rows \\(3\\) to \\(5\\) and columns \\(14\\) to \\(16\\) (the top edge of the \\(3\\)), the result will be:\n\n(im3[3:6,14:17] * top_edge).sum()\n\ntensor(2.9727)\n\n\nSimilarly, doing this in the window made by rows \\(7, 8, 9\\) and columns \\(14, 15, 16\\) (the bottom edge of \\(3\\)), we’ll get:\n\n(im3[7:10, 14:17] * top_edge).sum()\n\ntensor(-2.9570)\n\n\nThe positive pixels represent the “lighter” pixels and negative values represent the “darker” pixels in relative terms. Our kernel should be able to highlight the top edges of \\(3\\) by making the top edges lighter and the bottom edges darker.\nNote that to compute the convolutions, we are simply doing numpy’s element-wise multiplication followed by a sum.\nLet’s create a function which will compute the the convolutions over any \\(3 \\times 3\\) window with any \\(3 \\times 3\\) kernel.\n\n#row and col define the center coordinate of a 3x3 window\ndef apply_kernel(row, col, kernel): return (im3[row-1:row+2, col-1:col+2] * kernel).sum()\n\n\napply_kernel(4, 15, top_edge)\n\ntensor(2.9727)\n\n\nNext, we want to slide this kernel over the entire image. To do this, we’ll use list comprehension inside a list comprehension to create coordinates over which we want to move.\n\nrng = range(1, 27)\ntop_edge3 = tensor([[apply_kernel(i, j, top_edge) for j in rng] for i in rng])\nshow_image(top_edge3);\n\n\n\n\n\n\n\n\nOur simple little \\(3 \\times 3\\) kernel has managed to highlight the top edges of the digit \\(3\\) !\nHere’s a subtle thing that we need to notice.\n\ntop_edge3.shape\n\ntorch.Size([26, 26])\n\n\nThe shape of the output image is \\(26\\times 26\\) while the input image had \\(28\\times 28\\) dimensions. It is easy to see why the output dimensions change if we visualize the kernel sliding over all the \\(3\\times 3\\) windows in our input image. In general, if a kernel of dimension \\(f\\times f\\) convolve over an image of dimension \\(N\\times N\\), the output will have dimensions \\((N-f+1)\\times (N-f+1)\\).\nLet’s repeat the same exercise using another kernel which should be able to highlight the left edges.\n\nleft_edge = tensor([[-1, 0, 1],\n                    [-1, 0, 1],\n                    [-1, 0, 1]]).float()\nshow_image(left_edge, noframe=False);\n\n\n\n\n\n\n\n\n\nleft_edge3 = tensor([[apply_kernel(i, j, left_edge) for j in rng] for i in rng])\nshow_image(left_edge3);\n\n\n\n\n\n\n\n\nAnd it works as expected.\n\n\nConvolutions in PyTorch\nThe convolutions that we implemented in python above are quite slow.\n\n%timeit -n 1 tensor([[apply_kernel(i, j, left_edge) for j in rng] for i in rng])\n\n9.7 ms ± 5.35 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nPyTorch has an Conv2d class that is optimized for convolutions and offers added features and benefits.\n\ninp = im3[None,None,:,:].float()\n\nconv2d expects the input image to be a rank-four tensor where the dimensions correspond to batch, channel, height, width respectively. Since we are only considering a single image, our batch size (first dimension) is one, and we are working with black-and-white images, our images have a single channel. Colored images have three channels: red, green, and blue.\n\n%timeit -n 100 F.conv2d(inp, left_edge[None,None])\n\nThe slowest run took 479.87 times longer than the fastest. This could mean that an intermediate result is being cached.\n887 µs ± 2.1 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nConvolutions over a batch of images\nWhen training our network, we pass the input images to the models as batches. conv2d allows us to conveniently implement the convolutions using multiple kernels simultaneously over a batch of images.\nLet’s make two more kernels to detect diagnals in the image.\n\ndiag1_edge = tensor([[ 0,-1, 1],\n                     [-1, 1, 0],\n                     [ 1, 0, 0]]).float()\n\n\nshow_image(diag1_edge, noframe=False);\n\n\n\n\n\n\n\n\n\ndiag2_edge = tensor([[ 1,-1, 0],\n                     [ 0, 1,-1],\n                     [ 0, 0, 1]]).float()\n\n\nshow_image(diag2_edge);\n\n\n\n\n\n\n\n\nLet’s create a batch of 16 images.\n\nxb = x_imgs[:16][:, None]\nxb.shape\n\ntorch.Size([16, 1, 28, 28])\n\n\nAnd stack all our kernels together\n\nedge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])[:, None]\nedge_kernels.shape\n\ntorch.Size([4, 1, 3, 3])\n\n\nNow, we pass the batch and kernel to conv2d\n\nbatch_features = F.conv2d(xb, edge_kernels)\nbatch_features.shape\n\ntorch.Size([16, 4, 26, 26])\n\n\nThe output indicates that we have \\(16\\) images in the batch, \\(4\\) filters, and each image is of dimension \\(26\\times26\\).\nLet’s see what the kernels detect when applied on a particular image\n\nx = xb[1, 0]\nshow_image(x);\n\n\n\n\n\n\n\n\n\nshow_images([batch_features[1,i] for i in range(4)]);\n\n\n\n\n\n\n\n\n\n\nStrides and Padding\nUsing the convolution operation, we lose some pixels from our image, effectively losing data, which is never good. Using appropriate padding, we can ensure that the output activation map is the same size as the input image.  &gt;Figure 4: Padding\nZero padding is a common approach where the input outside the valid range is \\(0\\). If we pad the images so that the output dimensions are the same as the input dimensions, then it’s called valid padding. If we add a kernel of size \\(f\\times f\\) (with \\(f\\) an odd number), the necessary padding on each side to keep the same shape is \\(f//2\\).\nSo far, we’ve seen our kernel slide over the image grid by one unit, or in other words, it has a stride of 1. However, a kernel can have a stride greater than \\(1\\) as well. A stride of 2 means creating roughly half the number of outputs.\n &gt; Figure 5: stride-2 convolutions\nIf we have an image of size \\(n\\times n\\), a kernel of size \\(f\\times f\\), padding of \\(p\\) pixels on each side, and a stride of \\(s\\) then the output of the convolution operation will have a size of \\((\\frac{n-f+2p}{s} + 1) \\times (\\frac{n-f+2p}{s} + 1)\\).\n &gt; Figure 6: With a \\(5\\times 5\\) input, \\(4\\times 4\\) kernel, and \\(2\\) pixels of padding, we end up with a \\(6\\times 6\\) activation map.\n\n\nCreating a CNN\nWe are now able to create and train a Convolutional Neural Network. But before that, let’s recall how we’d build a simple one-layer MLP using nn.Sequential.\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\n\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\nWhat if we use the same idea to build a CNN as well.\n\nbroken_cnn = nn.Sequential(\n    nn.Conv2d(1, 30, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(30, 10, kernel_size=3, padding=1)\n)\n\n\nbroken_cnn(xb).shape\n\ntorch.Size([16, 10, 28, 28])\n\n\nWe want to have 10 output channels for each of the 16 images in the batch which isn’t the case here.\nTo make our ConvNet architecture, we’ll first create a conv function with appropriate input channels, output channels, stride, kernel size, and padding which returns a sequential model with an optional activation function.\n\ndef conv(ni, nf, ks=3, stride=2, act=True):\n    res = nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\nand then put those conv functions together to get our ConvNet architecture. The nn.Flatten() removes the unneccesary unit axes. The commented dimensions represent the dimensions of the output from that conv layer.\n\nsimple_cnn = nn.Sequential(\n    conv(1, 4),                 #14x14\n    conv(4, 8),                 #7x7\n    conv(8, 16),                #4x4\n    conv(16, 16),               #2x2\n    conv(16, 10, act=False),    #1x1\n    nn.Flatten()\n)\n\n\nsimple_cnn(xb).shape\n\ntorch.Size([16, 10])\n\n\nNow lets create our Datasets and DataLoaders to create batches for training.\n\nx_imgs = x_train.view(-1, 1, 28, 28)\nxv_imgs = x_valid.view(-1, 1, 28, 28)\ntrain_ds, valid_ds = Dataset(x_imgs, y_train), Dataset(xv_imgs, y_valid)\n\n\ndef_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef to_device(x, device=def_device):\n    if isinstance(x, torch.Tensor): return x.to(device)\n    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n    return type(x)(to_device(o, device) for o in x)\n\ndef collate_device(b): return to_device(default_collate(b))\n\n\nbs = 256\nlr = 0.4\ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, bs, collate_fn=collate_device)\nopt = optim.SGD(simple_cnn.parameters(), lr=lr)\n\nNotice that we move our model (and its parameters) to the gpu (if its available) to speed up training.\nWe’ll be using accuracy as a metric.\nLet’s call fit and train for 5 epochs.\n\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\nepoch:0, loss:0.9480767825126648, accuracy:0.715\nepoch:1, loss:0.26140550297498705, accuracy:0.9215\nepoch:2, loss:0.20503890758752824, accuracy:0.9388\nepoch:3, loss:0.13651981556415557, accuracy:0.9597\nepoch:4, loss:0.12075043138265609, accuracy:0.9665\n\n\nand it seems to be working fine.\nNow let’s reduce the learning rate to 0.1 and train again for 5 epochs.\n\nopt = optim.SGD(simple_cnn.parameters(), lr=lr/4)\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\nepoch:0, loss:0.09355406395196915, accuracy:0.9733\nepoch:1, loss:0.08985215668007732, accuracy:0.9746\nepoch:2, loss:0.08949725909978151, accuracy:0.9743\nepoch:3, loss:0.08952338592857123, accuracy:0.9743\nepoch:4, loss:0.10270511233061552, accuracy:0.9697\n\n\n… and accuracy has improved to 0.97!\nThat was all about building and training ConvNets from the ground up.\nThankyou for reading."
  },
  {
    "objectID": "blog/posts/ConvNets/index.html",
    "href": "blog/posts/ConvNets/index.html",
    "title": "Convolutional Neural Networks from Scratch",
    "section": "",
    "text": "This blog post is a code-first introduction to Convolutional Networks (ConvNets). We will build our own ConvNet from scratch and understand the fundamentals of how and why they work so well, particularly for image processing tasks.\nNB: This blog uses some functions from the little_ai library I am currently building.\n\nImporting Libraries\n\nimport torch\nfrom torch import nn\n\nfrom torch.utils.data import default_collate\nfrom typing import Mapping\n\nfrom little_ai.training import *\nfrom little_ai.datasets import *\n\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\nimport pandas as pd,matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\nfrom torch import tensor\nimport torch.nn.functional as F\nfrom torch import optim\n\nfrom torch.utils.data import DataLoader\nfrom typing import Mapping\n\n\nmpl.rcParams['image.cmap'] = 'gray'\n\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n\n\n\nWhy ConvNets\n &gt; Figure 1: A simple neural net with two hidden layers\nImages have properties that necessitate the need for specialized architectures.\n\nThey are high-dimensional. RGB images of dimensions \\(224\\times224\\) are very common (i.e., \\(150, 528\\) input dimensions). Hidden layers in fully connected networks are generally larger than the input size, so even for a shallow network, the number of weights would exceed \\(150,528^2\\) or \\(22\\) billion. This poses obvious practical problems in terms of the required training data, memory, and computation.\nNearby image pixels are statistically related. However, fully connected networks have no notion of “nearby” and treat the relation between every input equally.\nThe interpretation of an image is stable under geometric transformations. An image of a tree is still an image of a tree if we shift it leftwards by a few pixels. However, this shift is stable under geometric transformations. Hence, a fully connected model must learn the patterns of pixels that signify a tree separately at every position, which is inefficient.\n\nConvolutional layers process each local image region independently, using parameters shared across the whole image. They use fewer parameters than fully connected layers, exploit the spatial relationships between nearby pixels, and don’t have to re-learn the interpretation of pixels at every position.\n &gt; Figure 2: Architecture of a simple ConvNet\n\n\nConvolution Operation\nConvolutional layers perform the convolution operation using kernels (or filters) which is simply a matrix, over an image. Convolution is a simple mathematical operation that involves addition and multiplication.\n\n\n\nConvolution Operation\n\n\n\nFigure 3: Convolution operation using a \\(2\\times2\\) kernel over a \\(3\\times4\\) image that results in a \\(2\\times3\\) output matrix. The kernel slides over the image, adding the products of the overlapping matrix values. Source: Deep Learning by Goodfellow, et al.\n\nNow, let’s implement our own convolution operation using numpy. We’ll choose an image from the training set and create a simple \\(3\\times 3\\) matrix as our kernel and, as we’ll see, the convolution operation with the right choice of the kernel will be able to identify some pattern in our image.\n\nx_imgs = x_train.view(-1, 28, 28)\nxv_imgs = x_valid.view(-1, 28, 28)\n\n\nmpl.rcParams['figure.dpi'] = 30\n\n\nim3 = x_imgs[7]\nshow_image(im3);\n\n\n\n\n\n\n\n\nLet’s create a simple \\(3\\times3\\) kernel (people in the computer vision world call it a kernel; essentially it’s just a tensor).\nNB: Actually, the values of kernels are not entered manually but are learned as parameters during training.\n\ntop_edge = tensor([[-1, -1, -1], \n                  [0, 0, 0], \n                  [1, 1, 1]]).float()\n\n\nshow_image(top_edge, noframe=False);\n\n\n\n\n\n\n\n\nThis kernel will slide along all the \\(3\\times 3\\) windows in our images and compute the convolutions. If \\(3\\times 3\\)` window looks like this:\n\\[\\begin{matrix} a & b & c \\\\ d & e & f \\\\ g & h & i\\end{matrix}\\]\nthen the result will be \\(-a -b -c +g+h+i\\)\n\ndf = pd.DataFrame(im3[:13,:23])\ndf.style.format(precision=2).set_properties(**{'font-size':'7pt'}).background_gradient('Greys')\n\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n\n0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n2\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n3\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n4\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n5\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.15\n0.17\n0.41\n1.00\n0.99\n0.99\n0.99\n0.99\n0.99\n0.68\n0.02\n0.00\n\n\n6\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.17\n0.54\n0.88\n0.88\n0.98\n0.99\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.62\n0.05\n\n\n7\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.70\n0.98\n0.98\n0.98\n0.98\n0.99\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.23\n\n\n8\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.43\n0.98\n0.98\n0.90\n0.52\n0.52\n0.52\n0.52\n0.74\n0.98\n0.98\n0.98\n0.98\n0.23\n\n\n9\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.11\n0.11\n0.09\n0.00\n0.00\n0.00\n0.00\n0.05\n0.88\n0.98\n0.98\n0.67\n0.03\n\n\n10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\n0.95\n0.98\n0.98\n0.56\n0.00\n\n\n11\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.34\n0.74\n0.98\n0.98\n0.98\n0.05\n0.00\n\n\n12\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.36\n0.83\n0.96\n0.98\n0.98\n0.98\n0.80\n0.04\n0.00\n\n\n\n\n\n\nIf we compute the convolution over the \\(3\\times 3\\) window along rows \\(3\\) to \\(5\\) and columns \\(14\\) to \\(16\\) (the top edge of the \\(3\\)), the result will be:\n\n(im3[3:6,14:17] * top_edge).sum()\n\ntensor(2.9727)\n\n\nSimilarly, doing this in the window made by rows \\(7, 8, 9\\) and columns \\(14, 15, 16\\) (the bottom edge of \\(3\\)), we’ll get:\n\n(im3[7:10, 14:17] * top_edge).sum()\n\ntensor(-2.9570)\n\n\nThe positive pixels represent the “lighter” pixels and negative values represent the “darker” pixels in relative terms. Our kernel should be able to highlight the top edges of \\(3\\) by making the top edges lighter and the bottom edges darker.\nNote that to compute the convolutions, we are simply doing numpy’s element-wise multiplication followed by a sum.\nLet’s create a function which will compute the the convolutions over any \\(3 \\times 3\\) window with any \\(3 \\times 3\\) kernel.\n\n#row and col define the center coordinate of a 3x3 window\ndef apply_kernel(row, col, kernel): return (im3[row-1:row+2, col-1:col+2] * kernel).sum()\n\n\napply_kernel(4, 15, top_edge)\n\ntensor(2.9727)\n\n\nNext, we want to slide this kernel over the entire image. To do this, we’ll use list comprehension inside a list comprehension to create coordinates over which we want to move.\n\nrng = range(1, 27)\ntop_edge3 = tensor([[apply_kernel(i, j, top_edge) for j in rng] for i in rng])\nshow_image(top_edge3);\n\n\n\n\n\n\n\n\nOur simple little \\(3 \\times 3\\) kernel has managed to highlight the top edges of the digit \\(3\\) !\nHere’s a subtle thing that we need to notice.\n\ntop_edge3.shape\n\ntorch.Size([26, 26])\n\n\nThe shape of the output image is \\(26\\times 26\\) while the input image had \\(28\\times 28\\) dimensions. It is easy to see why the output dimensions change if we visualize the kernel sliding over all the \\(3\\times 3\\) windows in our input image. In general, if a kernel of dimension \\(f\\times f\\) convolve over an image of dimension \\(N\\times N\\), the output will have dimensions \\((N-f+1)\\times (N-f+1)\\).\nLet’s repeat the same exercise using another kernel which should be able to highlight the left edges.\n\nleft_edge = tensor([[-1, 0, 1],\n                    [-1, 0, 1],\n                    [-1, 0, 1]]).float()\nshow_image(left_edge, noframe=False);\n\n\n\n\n\n\n\n\n\nleft_edge3 = tensor([[apply_kernel(i, j, left_edge) for j in rng] for i in rng])\nshow_image(left_edge3);\n\n\n\n\n\n\n\n\nAnd it works as expected.\n\n\nConvolutions in PyTorch\nThe convolutions that we implemented in python above are quite slow.\n\n%timeit -n 1 tensor([[apply_kernel(i, j, left_edge) for j in rng] for i in rng])\n\n7.82 ms ± 1.23 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nPyTorch has an Conv2d class that is optimized for convolutions and offers added features and benefits.\n\ninp = im3[None,None,:,:].float()\n\nconv2d expects the input image to be a rank-four tensor where the dimensions correspond to batch, channel, height, width respectively. Since we are only considering a single image, our batch size (first dimension) is one, and we are working with black-and-white images, our images have a single channel. Colored images have three channels: red, green, and blue.\n\n%timeit -n 100 F.conv2d(inp, left_edge[None,None])\n\n23 µs ± 10.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nConvolutions over a batch of images\nWhen training our network, we pass the input images to the models as batches. conv2d allows us to conveniently implement the convolutions using multiple kernels simultaneously over a batch of images.\nLet’s make two more kernels to detect diagnals in the image.\n\ndiag1_edge = tensor([[ 0,-1, 1],\n                     [-1, 1, 0],\n                     [ 1, 0, 0]]).float()\n\n\nshow_image(diag1_edge, noframe=False);\n\n\n\n\n\n\n\n\n\ndiag2_edge = tensor([[ 1,-1, 0],\n                     [ 0, 1,-1],\n                     [ 0, 0, 1]]).float()\n\n\nshow_image(diag2_edge);\n\n\n\n\n\n\n\n\nLet’s create a batch of 16 images.\n\nxb = x_imgs[:16][:, None]\nxb.shape\n\ntorch.Size([16, 1, 28, 28])\n\n\nAnd stack all our kernels together\n\nedge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])[:, None]\nedge_kernels.shape\n\ntorch.Size([4, 1, 3, 3])\n\n\nNow, we pass the batch and kernel to conv2d\n\nbatch_features = F.conv2d(xb, edge_kernels)\nbatch_features.shape\n\ntorch.Size([16, 4, 26, 26])\n\n\nThe output indicates that we have \\(16\\) images in the batch, \\(4\\) filters, and each image is of dimension \\(26\\times26\\).\nLet’s see what the kernels detect when applied on a particular image\n\nx = xb[1, 0]\nshow_image(x);\n\n\n\n\n\n\n\n\n\nshow_images([batch_features[1,i] for i in range(4)]);\n\n\n\n\n\n\n\n\n\n\nStrides and Padding\nUsing the convolution operation, we lose some pixels from our image, effectively losing data, which is never good. Using appropriate padding, we can ensure that the output activation map is the same size as the input image.  &gt;Figure 4: Padding\nZero padding is a common approach where the input outside the valid range is \\(0\\). If we pad the images so that the output dimensions are the same as the input dimensions, then it’s called valid padding. If we add a kernel of size \\(f\\times f\\) (with \\(f\\) an odd number), the necessary padding on each side to keep the same shape is \\(f//2\\).\nSo far, we’ve seen our kernel slide over the image grid by one unit, or in other words, it has a stride of 1. However, a kernel can have a stride greater than \\(1\\) as well. A stride of 2 means creating roughly half the number of outputs.\n &gt; Figure 5: stride-2 convolutions\nIf we have an image of size \\(n\\times n\\), a kernel of size \\(f\\times f\\), padding of \\(p\\) pixels on each side, and a stride of \\(s\\) then the output of the convolution operation will have a size of \\((\\frac{n-f+2p}{s} + 1) \\times (\\frac{n-f+2p}{s} + 1)\\).\n &gt; Figure 6: With a \\(5\\times 5\\) input, \\(4\\times 4\\) kernel, and \\(2\\) pixels of padding, we end up with a \\(6\\times 6\\) activation map.\n\n\nCreating a CNN\nWe are now able to create and train a Convolutional Neural Network. But before that, let’s recall how we’d build a simple one-layer MLP using nn.Sequential.\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\n\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\nWhat if we use the same idea to build a CNN as well.\n\nbroken_cnn = nn.Sequential(\n    nn.Conv2d(1, 30, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(30, 10, kernel_size=3, padding=1)\n)\n\n\nbroken_cnn(xb).shape\n\ntorch.Size([16, 10, 28, 28])\n\n\nWe want to have 10 output channels for each of the 16 images in the batch which isn’t the case here.\nTo make our ConvNet architecture, we’ll first create a conv function with appropriate input channels, output channels, stride, kernel size, and padding which returns a sequential model with an optional activation function.\n\ndef conv(ni, nf, ks=3, stride=2, act=True):\n    res = nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\nand then put those conv functions together to get our ConvNet architecture. The nn.Flatten() removes the unneccesary unit axes. The commented dimensions represent the dimensions of the output from that conv layer.\n\nsimple_cnn = nn.Sequential(\n    conv(1, 4),                 #14x14\n    conv(4, 8),                 #7x7\n    conv(8, 16),                #4x4\n    conv(16, 16),               #2x2\n    conv(16, 10, act=False),    #1x1\n    nn.Flatten()\n)\n\n\nsimple_cnn(xb).shape\n\ntorch.Size([16, 10])\n\n\nNow lets create our Datasets and DataLoaders to create batches for training.\n\nx_imgs = x_train.view(-1, 1, 28, 28)\nxv_imgs = x_valid.view(-1, 1, 28, 28)\ntrain_ds, valid_ds = Dataset(x_imgs, y_train), Dataset(xv_imgs, y_valid)\n\n\ndef_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef to_device(x, device=def_device):\n    if isinstance(x, torch.Tensor): return x.to(device)\n    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n    return type(x)(to_device(o, device) for o in x)\n\ndef collate_device(b): return to_device(default_collate(b))\n\n\nbs = 256\nlr = 0.4\ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, bs, collate_fn=collate_device)\nopt = optim.SGD(simple_cnn.parameters(), lr=lr)\n\nNotice that we move our model (and its parameters) to the gpu (if its available) to speed up training.\nWe’ll be using accuracy as a metric.\nLet’s call fit and train for 5 epochs.\n\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\nepoch:0, loss:0.3939493108987808, accuracy:0.8802\nepoch:1, loss:0.1568004638493061, accuracy:0.9541\nepoch:2, loss:0.14168763396739958, accuracy:0.9604\nepoch:3, loss:0.1379448724284768, accuracy:0.9602\nepoch:4, loss:0.12180517784804105, accuracy:0.9635\n\n\nand it seems to be working fine.\nNow let’s reduce the learning rate to 0.1 and train again for 5 epochs.\n\nopt = optim.SGD(simple_cnn.parameters(), lr=lr/4)\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\nepoch:0, loss:0.08432787099350243, accuracy:0.9761\nepoch:1, loss:0.08465766663141548, accuracy:0.9762\nepoch:2, loss:0.08055750963222236, accuracy:0.976\nepoch:3, loss:0.08232340762391686, accuracy:0.9753\nepoch:4, loss:0.08031454239357262, accuracy:0.976\n\n\n… and accuracy has improved to 0.97!\nThat was all about building and training ConvNets from the ground up.\nThankyou for reading."
  },
  {
    "objectID": "blog/posts/stable-diffusion/Building_Stable_Diffusion_from_its_Components.html",
    "href": "blog/posts/stable-diffusion/Building_Stable_Diffusion_from_its_Components.html",
    "title": "Building Stable Diffusion from its Components",
    "section": "",
    "text": "A code-first introduction to Stable Diffusion using the Diffusers library.\n\nImporting Libraries\n\n!pip install -Uq diffusers transformers fastcore\n\n\nimport logging\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom fastcore.all import concat\nfrom huggingface_hub import notebook_login\nfrom PIL import Image\n\nlogging.disable(logging.WARNING)\n\ntorch.manual_seed(1)\nif not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\n\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\n\n\nStable Diffusion\nBefore diving into the code, let’s first take a bird’s eye view of how Stable Diffusion works. Stable Diffusion is a latent diffusion algorithm, meaning it works with the images in latent space, that is compresses the images into a smaller dimesnion using an autoencoder. This makes the process both faster and memory efficient. In the forward process of diffusion, Gaussian noise is added to the images using an encoder over a number of steps till the images is indistinguishable from white noise. In the backward process the images are iteratively denoised to generate new samples. The denoising is done using a unet model that predicts and subtracts noise which is then subtracted from the image.\n\n\nThe Diffusion Pipeline\nDiffusers library by Hugging Face is arguably the best way to use Stable Diffusion. The Diffusers library uses StableDiffusionPipeline, which is an end-to-end diffusion inference pipeline to combine all the steps in the diffusion process to easily do inference with the model. However, in this tutorial we will not use the pipeline directly ourselves but rather build it ourselves from its components to get a better understanding of stable diffusion works under the hood. This also gives us more control over the output generated by the model.\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", variant=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")\n\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\n\n\n\n\n\nLooking inside the pipeline\nThe diffusion pipeline combines all the components of stable diffusion for convenient inference. Let’s build the pipeline back by putting together its components.\nThe main componets of a latent diffusion algorithm are:\n\nAn autoencoder (VAE)- for compressing the image in the latent space\nA u-net- for predicting noise in the latent image\nA text encoder: for creating embeddings of text and images.\n\nThe output of a u-net, the noise residual, is used for gradual denoising of the latent image representation via a scheduler algorithm.\n\ndel pipe\n\nFirst, we need a text tokenizer and text encoder. Stable Diffusion uses the CLIP encoder by Open AI, so we’ll be using its weights:\n\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\nNext, we’ll use the vae and unet models.\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\n\n# Here we use a different VAE to the original release, which has been fine-tuned for more steps\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\nThe standard pipeline used the PNDM scheduler, but we’ll use the K-LMS scheduler. We need to use the same noising schedule as was used during training. The schedule is defined by the time step at which the noise is added and the amount of noise that is added, which is given by the beta parameters. For the K-LMS scheduler, this is how the betas evolve:\n\nbeta_start, beta_end = 0.00085, 0.012\nplt.plot(torch.linspace(beta_start ** 0.5, beta_end ** 0.5, 1000) ** 2)\nplt.xlabel('Timestep')\nplt.ylabel('β')\n\nText(0, 0.5, 'β')\n\n\n\n\n\n\n\n\n\n\nfrom diffusers import LMSDiscreteScheduler\n\n\nscheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule='scaled_linear', num_train_timesteps=1000)\n\nWe’ll now define the parameters we’ll use for generation\n\nprompt = \"Batman drinking coffee while sitting in a cafe\"\n\nheight = 512\nwidth = 512\nnum_inference_steps = 70\nguidance_scale = 7.5\nbatch_size = 1\n\nNext, we tokenize the prompt. To ensure that we have same number of tokens for each prompt, we’ll use max length padding and truncation.\n\ntext_input = tokenizer(prompt, padding='max_length', max_length=tokenizer.model_max_length, truncation=True, return_tensors='pt')\ntext_input['input_ids']\n\ntensor([[49406,  7223,  5778,  2453,  1519,  4919,   530,   320,  4979, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\n\n\n\ntokenizer.decode(49407), tokenizer.decode(49406)\n\n('&lt;|endoftext|&gt;', '&lt;|startoftext|&gt;')\n\n\n\ntext_input['attention_mask']\n\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]])\n\n\nThe text encoder gives us the embeddings for the text prompt we have used.\n\ntext_embeddings = text_encoder(text_input.input_ids.to('cuda'))[0].half()\ntext_embeddings.shape\n\ntorch.Size([1, 77, 768])\n\n\nWe’ll also get the embeddings required for unconditional generation, which is achieved with an empty string; the model is free to go anywhere as long as the results as reasonable. These embeddings will be used for classifier-free guidance.\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(['']*batch_size, padding='max_length',max_length=max_length, return_tensors='pt')\nuncond_embeddings = text_encoder(uncond_input.input_ids.to('cuda'))[0].half()\nuncond_embeddings.shape\n\ntorch.Size([1, 77, 768])\n\n\nFor classifier-free guidance, we need to do two forward passes. One with the conditioned input (text_embeddings), and another with the unconditional embeddings (uncond_embeddings). In practice, we can concatenate both into a single batch to avoid doing two forward passes.\n\ntext_embeddings = torch.cat([text_embeddings, uncond_embeddings])\ntext_embeddings.shape\n\ntorch.Size([2, 77, 768])\n\n\nTo start the denoising process, we’ll start from pure Gaussian noise. These are our initial latents.\n\ntorch.manual_seed(42)\nlatents = torch.randn((batch_size, unet.in_channels, height//8, width//8))\nlatents = latents.to('cuda').half()\nlatents.shape\n\nFutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((batch_size, unet.in_channels, height//8, width//8))\n\n\ntorch.Size([1, 4, 64, 64])\n\n\nWe initialize the scheduler with num_inference_steps. This will prepare the internal state to be used during denoising.\n\nscheduler.set_timesteps(num_inference_steps)\n\nWe scale the latents with the initial standard deviation required by the scheduler.\n\nlatents = latents * scheduler.init_noise_sigma\n\nWe are now ready to write the denoising loop. The timesteps go from 999 to 0 following a given schedule.\n\nscheduler.timesteps\n\ntensor([999.0000, 984.5217, 970.0435, 955.5652, 941.0870, 926.6087, 912.1304,\n        897.6522, 883.1739, 868.6957, 854.2174, 839.7391, 825.2609, 810.7826,\n        796.3043, 781.8261, 767.3478, 752.8696, 738.3913, 723.9130, 709.4348,\n        694.9565, 680.4783, 666.0000, 651.5217, 637.0435, 622.5652, 608.0870,\n        593.6087, 579.1304, 564.6522, 550.1739, 535.6957, 521.2174, 506.7391,\n        492.2609, 477.7826, 463.3044, 448.8261, 434.3478, 419.8696, 405.3913,\n        390.9131, 376.4348, 361.9565, 347.4783, 333.0000, 318.5217, 304.0435,\n        289.5652, 275.0869, 260.6087, 246.1304, 231.6522, 217.1739, 202.6956,\n        188.2174, 173.7391, 159.2609, 144.7826, 130.3044, 115.8261, 101.3478,\n         86.8696,  72.3913,  57.9130,  43.4348,  28.9565,  14.4783,   0.0000])\n\n\n\nscheduler.sigmas\n\ntensor([14.6146, 13.3974, 12.3033, 11.3184, 10.4301,  9.6279,  8.9020,  8.2443,\n         7.6472,  7.1044,  6.6102,  6.1594,  5.7477,  5.3709,  5.0258,  4.7090,\n         4.4178,  4.1497,  3.9026,  3.6744,  3.4634,  3.2680,  3.0867,  2.9183,\n         2.7616,  2.6157,  2.4794,  2.3521,  2.2330,  2.1213,  2.0165,  1.9180,\n         1.8252,  1.7378,  1.6552,  1.5771,  1.5031,  1.4330,  1.3664,  1.3030,\n         1.2427,  1.1852,  1.1302,  1.0776,  1.0272,  0.9788,  0.9324,  0.8876,\n         0.8445,  0.8029,  0.7626,  0.7236,  0.6858,  0.6490,  0.6131,  0.5781,\n         0.5438,  0.5102,  0.4770,  0.4443,  0.4118,  0.3795,  0.3470,  0.3141,\n         0.2805,  0.2455,  0.2084,  0.1672,  0.1174,  0.0292,  0.0000])\n\n\n\nplt.plot(scheduler.timesteps, scheduler.sigmas[:-1])\n\n\n\n\n\n\n\n\n\nfrom tqdm.auto import tqdm\n\n\nfor i, t in enumerate(tqdm(scheduler.timesteps)):\n    input = torch.cat([latents]*2)\n    input = scheduler.scale_model_input(input, t)\n\n    #predict the noise residual\n    with torch.no_grad():\n        pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n\n    #perform guidance\n    pred_text, pred_uncond = pred.chunk(2)\n    pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n\n    #compute the previous 'noisy' sample\n    latents = scheduler.step(pred, t, latents).prev_sample\n\n\n\n\nNow, our latents contain the denoised representation of the image. We use the vae decoder to convert it back to the pixel-space.\n\nwith torch.no_grad(): image = vae.decode(1 / 0.18215 * latents).sample\n\nAnd finally, lets convert it back to PIL to display the image.\nThe arithmetic acrobats are done to convert the decoded image in a form that can be used correctly by the PIL.\n\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image[0].detach().cpu().permute(1, 2, 0).numpy()\nimage = (image * 255).round().astype(\"uint8\")\nImage.fromarray(image)\n\n\n\n\n\n\n\n\nVoila! Here’s our Batman with his coffee.\nThis was the text2image pipeline. Now, let’s build the img2img pipeline. Instead of starting with pure noise, we’ll start with a certain image and add noise to it. We skip the initial steps of the denoising process and pretend that the given image is what the algorithm came up with.\n\n!curl --output philosophical_dog.jpg 'https://puppytoob.com/wp-content/uploads/2013/04/Dogs_In_Wind_8.jpg'\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 25821  100 25821    0     0   134k      0 --:--:-- --:--:-- --:--:--  134k\n\n\n\ninput_image = Image.open('philosophical_dog.jpg').resize((512, 512))\ninput_image\n\n\n\n\n\n\n\n\nLet’s create a function to encode our image into the latent space using vae.\n\nfrom torchvision import transforms as tfms\n\n\ndef pil_to_latent(input_im):\n    # Single image -&gt; single latent in a batch (so size 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to('cuda').half()*2 - 1)\n    return 0.18215 * latent.latent_dist.sample()\n\n… and a function to get the image back from latent representation\n\ndef latents_to_pil(latents):\n    # batch of latents -&gt; list of images\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\n\nencoded = pil_to_latent(input_image)\nencoded.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\nNow, let’s repeat the process we did above, except that we will start with the input image and not with pure noise.\n\n#set parameters\nprompt = ['A painting in the style of Vincent Van Gogh']\nheight = 512\nwidth = 512\nnum_inference_steps = 70\nguidance_scale = 7.5\nbatch_size = 1\n\n# Prep text embeddings\ntext_input = tokenizer(prompt, padding='max_length', max_length=tokenizer.model_max_length, truncation=True, return_tensors='pt')\ntext_embeddings = text_encoder(text_input.input_ids.to('cuda'))[0].half()\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer([\"\"]*batch_size, padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\nuncond_embeddings = text_encoder(uncond_input.input_ids.to('cuda'))[0].half()\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n#Prep scheduler (set num of inference steps)\nscheduler.set_timesteps(num_inference_steps)\n\n#Prep latents\nstart_step = 30\nstart_sigma = scheduler.sigmas[start_step]\nnoise = torch.randn_like(encoded)\nlatents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\nlatents = latents.to('cuda').half()\n\n#Denoising loop\nfor i, t in enumerate(tqdm(scheduler.timesteps)):\n    if i &gt; start_step:\n        input = torch.concat([latents]*2)\n        input = scheduler.scale_model_input(input, t)\n\n        #predict noise residual\n        with torch.no_grad():\n            pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n\n        #perform guidance\n        pred_uncond, pred_text = pred.chunk(2)\n        pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n\n        #compute prev noisy sample\n        latents = scheduler.step(pred, t, latents).prev_sample\n\nlatents_to_pil(latents)[0]\n\n\n\n\n\n\n\n\n\n\n\nAnd here’s our dog, ruminating about the meaninglessness of life."
  },
  {
    "objectID": "blog/posts/stable-diffusion/stable-diffusion.html",
    "href": "blog/posts/stable-diffusion/stable-diffusion.html",
    "title": "Building Stable Diffusion from its Components",
    "section": "",
    "text": "A code-first introduction to Stable Diffusion using the Diffusers library.\n\nImporting Libraries\n\n!pip install -Uq diffusers transformers fastcore\n\n\nimport logging\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom fastcore.all import concat\nfrom huggingface_hub import notebook_login\nfrom PIL import Image\n\nlogging.disable(logging.WARNING)\n\ntorch.manual_seed(1)\nif not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\n\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\n\n\nStable Diffusion\nBefore diving into the code, let’s first take a bird’s eye view of how Stable Diffusion works. Stable Diffusion is a latent diffusion algorithm, meaning it works with the images in latent space, that is compresses the images into a smaller dimesnion using an autoencoder. This makes the process both faster and memory efficient. In the forward process of diffusion, Gaussian noise is added to the images using an encoder over a number of steps till the images is indistinguishable from white noise. In the backward process the images are iteratively denoised to generate new samples. The denoising is done using a unet model that predicts and subtracts noise which is then subtracted from the image.\n\n\n\nThe Diffusion Pipeline\nDiffusers library by Hugging Face is arguably the best way to use Stable Diffusion. The Diffusers library uses StableDiffusionPipeline, which is an end-to-end diffusion inference pipeline to combine all the steps in the diffusion process to easily do inference with the model. However, in this tutorial we will not use the pipeline directly ourselves but rather build it ourselves from its components to get a better understanding of how stable diffusion works under the hood. This also gives us more control over the output generated by the model.\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", variant=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")\n\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\n\n\n\n\n\nLooking inside the pipeline\nThe diffusion pipeline combines all the components of stable diffusion for convenient inference. Let’s build the pipeline back by putting together its components.\nThe main componets of a latent diffusion algorithm are:\n\nAn autoencoder (VAE)- for compressing the image in the latent space\nA u-net- for predicting noise in the latent image\nA text encoder: for creating embeddings of text and images.\n\nThe output of a u-net, the noise residual, is used for gradual denoising of the latent image representation via a scheduler algorithm.\n\ndel pipe\n\nFirst, we need a text tokenizer and text encoder. Stable Diffusion uses the CLIP encoder by Open AI, so we’ll be using its weights:\n\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\nNext, we’ll use the vae and unet models.\n\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\n\n\n# Here we use a different VAE to the original release, which has been fine-tuned for more steps\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\nThe standard pipeline used the PNDM scheduler, but we’ll use the K-LMS scheduler. We need to use the same noising schedule as was used during training. The schedule is defined by the time step at which the noise is added and the amount of noise that is added, which is given by the beta parameters. For the K-LMS scheduler, this is how the betas evolve:\n\nbeta_start, beta_end = 0.00085, 0.012\nplt.plot(torch.linspace(beta_start ** 0.5, beta_end ** 0.5, 1000) ** 2)\nplt.xlabel('Timestep')\nplt.ylabel('β')\n\nText(0, 0.5, 'β')\n\n\n\n\n\n\n\n\n\n\nfrom diffusers import LMSDiscreteScheduler\n\n\nscheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule='scaled_linear', num_train_timesteps=1000)\n\nWe’ll now define the parameters we’ll use for generation\n\nprompt = \"Batman drinking coffee while sitting in a cafe\"\n\nheight = 512\nwidth = 512\nnum_inference_steps = 70\nguidance_scale = 7.5\nbatch_size = 1\n\nNext, we tokenize the prompt. To ensure that we have same number of tokens for each prompt, we’ll use max length padding and truncation.\n\ntext_input = tokenizer(prompt, padding='max_length', max_length=tokenizer.model_max_length, truncation=True, return_tensors='pt')\ntext_input['input_ids']\n\ntensor([[49406,  7223,  5778,  2453,  1519,  4919,   530,   320,  4979, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\n\n\n\ntokenizer.decode(49407), tokenizer.decode(49406)\n\n('&lt;|endoftext|&gt;', '&lt;|startoftext|&gt;')\n\n\n\ntext_input['attention_mask']\n\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0]])\n\n\nThe text encoder gives us the embeddings for the text prompt we have used.\n\ntext_embeddings = text_encoder(text_input.input_ids.to('cuda'))[0].half()\ntext_embeddings.shape\n\ntorch.Size([1, 77, 768])\n\n\nWe’ll also get the embeddings required for unconditional generation, which is achieved with an empty string; the model is free to go anywhere as long as the results as reasonable. These embeddings will be used for classifier-free guidance.\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer(['']*batch_size, padding='max_length',max_length=max_length, return_tensors='pt')\nuncond_embeddings = text_encoder(uncond_input.input_ids.to('cuda'))[0].half()\nuncond_embeddings.shape\n\ntorch.Size([1, 77, 768])\n\n\nFor classifier-free guidance, we need to do two forward passes. One with the conditioned input (text_embeddings), and another with the unconditional embeddings (uncond_embeddings). In practice, we can concatenate both into a single batch to avoid doing two forward passes.\n\ntext_embeddings = torch.cat([text_embeddings, uncond_embeddings])\ntext_embeddings.shape\n\ntorch.Size([2, 77, 768])\n\n\nTo start the denoising process, we’ll start from pure Gaussian noise. These are our initial latents.\n\ntorch.manual_seed(42)\nlatents = torch.randn((batch_size, unet.in_channels, height//8, width//8))\nlatents = latents.to('cuda').half()\nlatents.shape\n\nFutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n  latents = torch.randn((batch_size, unet.in_channels, height//8, width//8))\n\n\ntorch.Size([1, 4, 64, 64])\n\n\nWe initialize the scheduler with num_inference_steps. This will prepare the internal state to be used during denoising.\n\nscheduler.set_timesteps(num_inference_steps)\n\nWe scale the latents with the initial standard deviation required by the scheduler.\n\nlatents = latents * scheduler.init_noise_sigma\n\nWe are now ready to write the denoising loop. The timesteps go from 999 to 0 following a given schedule.\n\nscheduler.timesteps\n\ntensor([999.0000, 984.5217, 970.0435, 955.5652, 941.0870, 926.6087, 912.1304,\n        897.6522, 883.1739, 868.6957, 854.2174, 839.7391, 825.2609, 810.7826,\n        796.3043, 781.8261, 767.3478, 752.8696, 738.3913, 723.9130, 709.4348,\n        694.9565, 680.4783, 666.0000, 651.5217, 637.0435, 622.5652, 608.0870,\n        593.6087, 579.1304, 564.6522, 550.1739, 535.6957, 521.2174, 506.7391,\n        492.2609, 477.7826, 463.3044, 448.8261, 434.3478, 419.8696, 405.3913,\n        390.9131, 376.4348, 361.9565, 347.4783, 333.0000, 318.5217, 304.0435,\n        289.5652, 275.0869, 260.6087, 246.1304, 231.6522, 217.1739, 202.6956,\n        188.2174, 173.7391, 159.2609, 144.7826, 130.3044, 115.8261, 101.3478,\n         86.8696,  72.3913,  57.9130,  43.4348,  28.9565,  14.4783,   0.0000])\n\n\n\nscheduler.sigmas\n\ntensor([14.6146, 13.3974, 12.3033, 11.3184, 10.4301,  9.6279,  8.9020,  8.2443,\n         7.6472,  7.1044,  6.6102,  6.1594,  5.7477,  5.3709,  5.0258,  4.7090,\n         4.4178,  4.1497,  3.9026,  3.6744,  3.4634,  3.2680,  3.0867,  2.9183,\n         2.7616,  2.6157,  2.4794,  2.3521,  2.2330,  2.1213,  2.0165,  1.9180,\n         1.8252,  1.7378,  1.6552,  1.5771,  1.5031,  1.4330,  1.3664,  1.3030,\n         1.2427,  1.1852,  1.1302,  1.0776,  1.0272,  0.9788,  0.9324,  0.8876,\n         0.8445,  0.8029,  0.7626,  0.7236,  0.6858,  0.6490,  0.6131,  0.5781,\n         0.5438,  0.5102,  0.4770,  0.4443,  0.4118,  0.3795,  0.3470,  0.3141,\n         0.2805,  0.2455,  0.2084,  0.1672,  0.1174,  0.0292,  0.0000])\n\n\n\nplt.plot(scheduler.timesteps, scheduler.sigmas[:-1])\n\n\n\n\n\n\n\n\n\nfrom tqdm.auto import tqdm\n\n\nfor i, t in enumerate(tqdm(scheduler.timesteps)):\n    input = torch.cat([latents]*2)\n    input = scheduler.scale_model_input(input, t)\n\n    #predict the noise residual\n    with torch.no_grad():\n        pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n\n    #perform guidance\n    pred_text, pred_uncond = pred.chunk(2)\n    pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n\n    #compute the previous 'noisy' sample\n    latents = scheduler.step(pred, t, latents).prev_sample\n\n\n\n\nNow, our latents contain the denoised representation of the image. We use the vae decoder to convert it back to the pixel-space.\n\nwith torch.no_grad(): image = vae.decode(1 / 0.18215 * latents).sample\n\nAnd finally, lets convert it back to PIL to display the image.\nThe arithmetic acrobats are done to convert the decoded image in a form that can be used correctly by the PIL.\n\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image[0].detach().cpu().permute(1, 2, 0).numpy()\nimage = (image * 255).round().astype(\"uint8\")\nImage.fromarray(image)\n\n\n\n\n\n\n\n\nVoila! Here’s our Batman with his coffee.\nThis was the text2image pipeline. Now, let’s build the img2img pipeline. Instead of starting with pure noise, we’ll start with a certain image and add noise to it. We skip the initial steps of the denoising process and pretend that the given image is what the algorithm came up with.\n\n!curl --output philosophical_dog.jpg 'https://puppytoob.com/wp-content/uploads/2013/04/Dogs_In_Wind_8.jpg'\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 25821  100 25821    0     0   134k      0 --:--:-- --:--:-- --:--:--  134k\n\n\n\ninput_image = Image.open('philosophical_dog.jpg').resize((512, 512))\ninput_image\n\n\n\n\n\n\n\n\nLet’s create a function to encode our image into the latent space using vae.\n\nfrom torchvision import transforms as tfms\n\n\ndef pil_to_latent(input_im):\n    # Single image -&gt; single latent in a batch (so size 1, 4, 64, 64)\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to('cuda').half()*2 - 1)\n    return 0.18215 * latent.latent_dist.sample()\n\n… and a function to get the image back from latent representation\n\ndef latents_to_pil(latents):\n    # batch of latents -&gt; list of images\n    latents = (1 / 0.18215) * latents\n    with torch.no_grad():\n        image = vae.decode(latents).sample\n    image = (image / 2 + 0.5).clamp(0, 1)\n    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (image * 255).round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\n\nencoded = pil_to_latent(input_image)\nencoded.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\nNow, let’s repeat the process we did above, except that we will start with the input image and not with pure noise.\n\n#set parameters\nprompt = ['A painting in the style of Vincent Van Gogh']\nheight = 512\nwidth = 512\nnum_inference_steps = 70\nguidance_scale = 7.5\nbatch_size = 1\n\n# Prep text embeddings\ntext_input = tokenizer(prompt, padding='max_length', max_length=tokenizer.model_max_length, truncation=True, return_tensors='pt')\ntext_embeddings = text_encoder(text_input.input_ids.to('cuda'))[0].half()\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer([\"\"]*batch_size, padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\nuncond_embeddings = text_encoder(uncond_input.input_ids.to('cuda'))[0].half()\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n#Prep scheduler (set num of inference steps)\nscheduler.set_timesteps(num_inference_steps)\n\n#Prep latents\nstart_step = 30\nstart_sigma = scheduler.sigmas[start_step]\nnoise = torch.randn_like(encoded)\nlatents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([scheduler.timesteps[start_step]]))\nlatents = latents.to('cuda').half()\n\n#Denoising loop\nfor i, t in enumerate(tqdm(scheduler.timesteps)):\n    if i &gt; start_step:\n        input = torch.concat([latents]*2)\n        input = scheduler.scale_model_input(input, t)\n\n        #predict noise residual\n        with torch.no_grad():\n            pred = unet(input, t, encoder_hidden_states=text_embeddings).sample\n\n        #perform guidance\n        pred_uncond, pred_text = pred.chunk(2)\n        pred = pred_uncond + guidance_scale * (pred_text - pred_uncond)\n\n        #compute prev noisy sample\n        latents = scheduler.step(pred, t, latents).prev_sample\n\nlatents_to_pil(latents)[0]\n\n\n\n\n\n\n\n\n\n\n\nAnd here’s our dog, ruminating about the meaninglessness of life."
  },
  {
    "objectID": "blog/posts/ddpm/DDPM.html",
    "href": "blog/posts/ddpm/DDPM.html",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "",
    "text": "Implementing the famous “Denoising Diffusion Probabilistic Models” paper which caused the spark that resulted in the fire of image generation models that we see and use today."
  },
  {
    "objectID": "blog/posts/ddpm/DDPM.html#forward-process",
    "href": "blog/posts/ddpm/DDPM.html#forward-process",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "Forward process",
    "text": "Forward process\nIn the forward pass Gaussian noise is gradually added to the data according to a variance schedule \\(\\beta_{1},\\cdots,\\beta_{T}\\). The noise added increases with the timestep \\(t\\) that is, \\(\\beta_{t}\\) increases with \\(t\\).\nThe conditional noise is given by: \\[\nq(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)\n\\] , where \\(\\sqrt{1-\\beta_t}x_{t-1}\\) is the mean and \\(\\beta_tI\\) is the variance. The above equation allows us to sample \\(x_{t}\\) at any timestep \\(t\\). Define \\(\\alpha_t=1-\\beta_{t}\\) and \\(\\bar{\\alpha}_t  = \\prod_{s=1}^{t}\\alpha_s\\). Then, \\[\nq(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}} x_0, (1-\\bar{\\alpha_t})I)\n\\]"
  },
  {
    "objectID": "blog/posts/ddpm/DDPM.html#reverse-process",
    "href": "blog/posts/ddpm/DDPM.html#reverse-process",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "Reverse Process",
    "text": "Reverse Process\nIn the reverse process, we estimate: \\[\np_{\\theta}(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\boldsymbol{\\mu}_\\theta(x_t, t), \\boldsymbol{\\Sigma}_\\theta(x_t, t))\n\\] ,where \\(\\boldsymbol{\\mu_\\theta}\\) and \\(\\boldsymbol{\\Sigma}_\\theta\\) are neural nets that take in the noised inputs \\(x_t\\) and the timestep \\(t\\) as the inputs and \\(\\theta\\) are the model weights. In DDPM however, \\(\\boldsymbol{\\Sigma}_\\theta\\) is ignored and treated as a \\(\\beta_t\\) dependent timestep.\n\\(\\boldsymbol{\\mu_\\theta}\\) can be reparametrized as \\[\n\\boldsymbol{\\mu_\\theta}(x_t, t) = \\frac{1}{\\sqrt{\\alpha}_t}\\bigg(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_\\theta(x_t, t)\\bigg)\n\\] , where \\(\\epsilon_\\theta\\) is the neural net that predicts noise from \\(x_t\\). In diffusion models, a unet architecture is used for predicting noise.\nWe can reparametrize \\(x_t\\) as: \\[\nx_t(x_0, \\epsilon) = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon\n\\] , where \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).\nThe loss function is simply a MSE loss given by: \\[\nL(\\theta) := \\mathbb{E}_{t,x_0, \\epsilon}\\bigg[||\\epsilon - \\epsilon_0(\\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1 - \\bar{\\alpha_t}}\\epsilon, t) ||^2\\bigg]\n\\]\nAs described in the paper, \\(x_0\\) can be approximated as: \\[\nx_0 \\approx \\hat{x}_0 = (x_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_{\\theta}(x_t))/\\sqrt{\\bar{\\alpha}_t}\n\\]\nTo sample \\(x_{t-1}\\sim p_\\theta(x_{t-1}|x_t)\\): \\[\nx_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\bigg(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha_t}}}\\epsilon_\\theta(x_t, t)\\bigg) + \\sigma_t z\n\\] , where \\(z \\sim \\mathcal{N}(0, I)\\)"
  },
  {
    "objectID": "blog/posts/ddpm/DDPM.html#training-algorithm",
    "href": "blog/posts/ddpm/DDPM.html#training-algorithm",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "Training Algorithm",
    "text": "Training Algorithm\n\nrepeat\n\\(x_0 \\sim q(x_0)\\)\n\\(t \\sim Uniform({1, \\cdots, T})\\)\n\\(\\epsilon \\sim \\mathcal{N}(0, I)\\)\nTake gradient descent step on \\(\\nabla_\\theta||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1 - \\bar{\\alpha_t}}\\epsilon, t)||^2\\)\nuntil converged"
  },
  {
    "objectID": "blog/posts/ddpm/DDPM.html#sampling-algorithm",
    "href": "blog/posts/ddpm/DDPM.html#sampling-algorithm",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "Sampling Algorithm",
    "text": "Sampling Algorithm\n\n\\(x_t \\sim \\mathcal{N}(0, I)\\)\nfor \\(t=T, \\cdots, 1\\) do\n\\(z \\sim\\mathcal{N}(0, I)\\) if \\(t&gt;1\\) else \\(z=0\\)\n\\(x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\bigg(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha_t}}}\\epsilon_\\theta(x_t, t)\\bigg) + \\sigma_t z\\)\nend for\nreturn \\(x_0\\)"
  },
  {
    "objectID": "blog/posts/ddpm/DDPM.html#import",
    "href": "blog/posts/ddpm/DDPM.html#import",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "Import",
    "text": "Import\n\nimport pickle,gzip,math,os,time,shutil,torch,random,logging\nimport fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom operator import attrgetter,itemgetter\nfrom functools import partial\nfrom copy import copy\nfrom contextlib import contextmanager\n\nfrom fastcore.foundation import L\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nfrom torch.utils.data import DataLoader,default_collate\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nfrom datasets import load_dataset,load_dataset_builder\n\nfrom little_ai.datasets import *\nfrom little_ai.conv import *\nfrom little_ai.learner import *\nfrom little_ai.activations import *\nfrom little_ai.init import *\nfrom little_ai.sgd import *\nfrom little_ai.resnet import *\nfrom little_ai.augment import *\n\n\nmpl.rcParams['image.cmap'] = 'gray'\nlogging.disable(logging.WARNING)"
  },
  {
    "objectID": "blog/posts/ddpm/DDPM.html#load-the-dataset",
    "href": "blog/posts/ddpm/DDPM.html#load-the-dataset",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "Load the Dataset",
    "text": "Load the Dataset\nWe’ll be using the popular fashion-mnist dataset\n\nx,y = 'image','label'\nname = \"fashion_mnist\"\ndsd = load_dataset(name)\n\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@inplace\ndef transformi(b): b[x] = [TF.resize(TF.to_tensor(o), (32,32)) for o in b[x]]\n\n\nset_seed(42)\nbs = 128\ntds = dsd.with_transform(transformi)\ndls = DataLoaders.from_dd(tds, bs, num_workers=8)\ndt = dls.train\nxb,yb = next(iter(dt))\nxb.shape,yb[:10]\n\n/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n\n(torch.Size([128, 1, 32, 32]), tensor([5, 7, 4, 7, 3, 8, 9, 5, 3, 1]))"
  },
  {
    "objectID": "blog/posts/ddpm/DDPM.html#create-model",
    "href": "blog/posts/ddpm/DDPM.html#create-model",
    "title": "Denoising Diffusion Probabilistic Models",
    "section": "Create model",
    "text": "Create model\nUnet model is used to predict noise from the image. We’ll import the unet model from Diffusers library.\n\nfrom diffusers import UNet2DModel\n\n/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  torch.utils._pytree._register_pytree_node(\n/usr/local/lib/python3.10/dist-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  torch.utils._pytree._register_pytree_node(\n\n\n\nmodel = UNet2DModel(in_channels=1, out_channels=1, block_out_channels=(32, 64, 128, 128))"
  },
  {
    "objectID": "blog/posts/style-transfer/Style_Transfer.html",
    "href": "blog/posts/style-transfer/Style_Transfer.html",
    "title": "Neural Style Transfer",
    "section": "",
    "text": "Style transfer is a technique where the visual style of an image is transferred onto the content of another image. The goal is to generate a new image that preserves the content details of one image while adopting the artistic style of another image. In this blog, we implement the A Neural Algorithm for Artistic Style paper by Gatys, et al. which introduced the neural style tranfer.\nNB: We’ll be using the little_ai library to implement the paper."
  },
  {
    "objectID": "blog/posts/style-transfer/Style_Transfer.html#load-vgg-network",
    "href": "blog/posts/style-transfer/Style_Transfer.html#load-vgg-network",
    "title": "Neural Style Transfer",
    "section": "Load VGG network",
    "text": "Load VGG network\n\nprint(timm.list_models('*vgg*'))\n\n['repvgg_a0', 'repvgg_a1', 'repvgg_a2', 'repvgg_b0', 'repvgg_b1', 'repvgg_b1g4', 'repvgg_b2', 'repvgg_b2g4', 'repvgg_b3', 'repvgg_b3g4', 'repvgg_d2se', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn']\n\n\n\nvgg16 = timm.create_model('vgg16', pretrained=True).to(def_device).features\n\n\nvgg16\n\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (18): ReLU(inplace=True)\n  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (25): ReLU(inplace=True)\n  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (27): ReLU(inplace=True)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)"
  },
  {
    "objectID": "blog/posts/style-transfer/Style_Transfer.html#normalize-images",
    "href": "blog/posts/style-transfer/Style_Transfer.html#normalize-images",
    "title": "Neural Style Transfer",
    "section": "Normalize Images",
    "text": "Normalize Images\nThis model expects images normalized with the same stats as those used during training, which in this case requires the stats of the ImageNet dataset.\n\nimagenet_mean = tensor([0.485, 0.456, 0.406])\nimagenet_std = tensor([0.229, 0.224, 0.225])\n\n\ndef normalize(img):\n    imagenet_mean = tensor([0.485, 0.456, 0.406])[:, None, None].to(def_device)\n    imagenet_std = tensor([0.229, 0.224, 0.225])[:, None, None].to(def_device)\n    return (img - imagenet_mean)/ imagenet_std\n\n\nnormalize(content_im).min(), normalize(content_im).max()\n\n(tensor(-2.1179, device='cuda:0'), tensor(2.6400, device='cuda:0'))\n\n\n\nnormalize(content_im).mean(dim=(1, 2))\n\ntensor([-0.9724, -0.9594, -0.4191], device='cuda:0')"
  },
  {
    "objectID": "blog/posts/style-transfer/Style_Transfer.html#getting-intermediate-representations",
    "href": "blog/posts/style-transfer/Style_Transfer.html#getting-intermediate-representations",
    "title": "Neural Style Transfer",
    "section": "Getting intermediate representations",
    "text": "Getting intermediate representations\nWe want to feed some data through the network, storing the outputs of different layers. Here’s one way to do this:\n\ndef calc_features(imgs, target_layers=(18, 25)):\n    x = normalize(imgs)\n    feats = []\n    for i, layer in enumerate(vgg16[:max(target_layers)+1]):\n        x = layer(x)\n        if i in target_layers:\n            feats.append(x.clone())\n    return feats\n\n\nfeats = calc_features(content_im)\n[f.shape for f in feats]\n\n[torch.Size([512, 32, 32]), torch.Size([512, 16, 16])]"
  },
  {
    "objectID": "blog/posts/style-transfer/Style_Transfer.html#trying-it-out",
    "href": "blog/posts/style-transfer/Style_Transfer.html#trying-it-out",
    "title": "Neural Style Transfer",
    "section": "Trying it out",
    "text": "Trying it out\n\nstyle_im = download_image(spiderweb_url).to(def_device)\nshow_image(style_im);\n\n\n\n\n\n\n\n\n\ndef calc_grams(img, target_layers=(1, 6, 11, 18, 25)):\n    return L(torch.einsum('chw, dhw -&gt; cd', x, x)/(x.shape[-1]*x.shape[-1])\n             for x in calc_features(img, target_layers))\n\n\nstyle_grams = calc_grams(style_im)\n\n\n[g.shape for g in style_grams]\n\n[torch.Size([64, 64]),\n torch.Size([128, 128]),\n torch.Size([256, 256]),\n torch.Size([512, 512]),\n torch.Size([512, 512])]\n\n\n\nstyle_grams.attrgot('shape')\n\n(#5) [torch.Size([64, 64]),torch.Size([128, 128]),torch.Size([256, 256]),torch.Size([512, 512]),torch.Size([512, 512])]\n\n\n\nclass StyleLossToTarget():\n    def __init__(self, target_im, target_layers=(1, 6, 11, 18, 25)):\n        fc.store_attr()\n        with torch.no_grad(): self.target_grams = calc_grams(target_im, self.target_layers)\n    def __call__(self, input_im):\n        return sum((f1-f2).pow(2).mean() for f1, f2 in\n                   zip(calc_grams(input_im, self.target_layers), self.target_grams))\n\n\nstyle_loss = StyleLossToTarget(style_im)\n\n\nstyle_loss(content_im)\n\ntensor(178.9305, device='cuda:0', grad_fn=&lt;AddBackward0&gt;)"
  }
]