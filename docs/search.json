[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kartikeya’s Musings",
    "section": "",
    "text": "Writing about my adventures as I dive deep into the world of Deep Learning, Literature, Movies, Music, and Life. You will find me writing technical blogs on deep learning, sharing what I learn, and occasionally reflecting on things as I experience them."
  },
  {
    "objectID": "blog/about.html",
    "href": "blog/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog/posts/post-with-code/index.html",
    "href": "blog/posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/welcome/index.html",
    "href": "blog/posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Neural Networks from Scratch\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nKartikeya Khare\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/ConvNets/ConvNets.html",
    "href": "blog/posts/ConvNets/ConvNets.html",
    "title": "Convolutional Neural Networks from Scratch",
    "section": "",
    "text": "This blog post is a code-first introduction to Convolutional Networks (ConvNets). We will build our own ConvNet from scratch and understand the fundamentals of how and why they work so well, particularly for image processing tasks.\nNB: This blog uses some functions from the little_ai library I am currently building.\n\nImporting Libraries\n\nimport torch\nfrom torch import nn\n\nfrom torch.utils.data import default_collate\nfrom typing import Mapping\n\nfrom little_ai.training import *\nfrom little_ai.datasets import *\n\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\nimport pandas as pd,matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\nfrom torch import tensor\nimport torch.nn.functional as F\nfrom torch import optim\n\nfrom torch.utils.data import DataLoader\nfrom typing import Mapping\n\n\nmpl.rcParams['image.cmap'] = 'gray'\n\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n\n\n\nWhy ConvNets\n &gt; Figure 1: A simple neural net with two hidden layers\nImages have properties that necessitate the need for specialized architectures.\n\nThey are high-dimensional. RGB images of dimensions \\(224\\times224\\) are very common (i.e., \\(150, 528\\) input dimensions). Hidden layers in fully connected networks are generally larger than the input size, so even for a shallow network, the number of weights would exceed \\(150,528^2\\) or \\(22\\) billion. This poses obvious practical problems in terms of the required training data, memory, and computation.\nNearby image pixels are statistically related. However, fully connected networks have no notion of “nearby” and treat the relation between every input equally.\nThe interpretation of an image is stable under geometric transformations. An image of a tree is still an image of a tree if we shift it leftwards by a few pixels. However, this shift is stable under geometric transformations. Hence, a fully connected model must learn the patterns of pixels that signify a tree separately at every position, which is inefficient.\n\nConvolutional layers process each local image region independently, using parameters shared across the whole image. They use fewer parameters than fully connected layers, exploit the spatial relationships between nearby pixels, and don’t have to re-learn the interpretation of pixels at every position.\n &gt; Figure 2: Architecture of a simple ConvNet\n\n\nConvolution Operation\nConvolutional layers perform the convolution operation using kernels (or filters) which is simply a matrix, over an image. Convolution is a simple mathematical operation that involves addition and multiplication.\n\n\n\nConvolution Operation\n\n\n\nFigure 3: Convolution operation using a \\(2\\times2\\) kernel over a \\(3\\times4\\) image that results in a \\(2\\times3\\) output matrix. The kernel slides over the image, adding the products of the overlapping matrix values. Source: Deep Learning by Goodfellow, et al.\n\nNow, let’s implement our own convolution operation using numpy. We’ll choose an image from the training set and create a simple \\(3\\times 3\\) matrix as our kernel and, as we’ll see, the convolution operation with the right choice of the kernel will be able to identify some pattern in our image.\n\nx_imgs = x_train.view(-1, 28, 28)\nxv_imgs = x_valid.view(-1, 28, 28)\n\n\nmpl.rcParams['figure.dpi'] = 30\n\n\nim3 = x_imgs[7]\nshow_image(im3);\n\n\n\n\n\n\n\n\nLet’s create a simple \\(3\\times3\\) kernel (people in the computer vision world call it a kernel; essentially it’s just a tensor).\nNB: Actually, the values of kernels are not entered manually but are learned as parameters during training.\n\ntop_edge = tensor([[-1, -1, -1], \n                  [0, 0, 0], \n                  [1, 1, 1]]).float()\n\n\nshow_image(top_edge, noframe=False);\n\n\n\n\n\n\n\n\nThis kernel will slide along all the \\(3\\times 3\\) windows in our images and compute the convolutions. If \\(3\\times 3\\)` window looks like this:\n\\[\\begin{matrix} a & b & c \\\\ d & e & f \\\\ g & h & i\\end{matrix}\\]\nthen the result will be \\(-a -b -c +g+h+i\\)\n\ndf = pd.DataFrame(im3[:13,:23])\ndf.style.format(precision=2).set_properties(**{'font-size':'7pt'}).background_gradient('Greys')\n\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n\n0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n2\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n3\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n4\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n5\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.15\n0.17\n0.41\n1.00\n0.99\n0.99\n0.99\n0.99\n0.99\n0.68\n0.02\n0.00\n\n\n6\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.17\n0.54\n0.88\n0.88\n0.98\n0.99\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.62\n0.05\n\n\n7\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.70\n0.98\n0.98\n0.98\n0.98\n0.99\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.23\n\n\n8\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.43\n0.98\n0.98\n0.90\n0.52\n0.52\n0.52\n0.52\n0.74\n0.98\n0.98\n0.98\n0.98\n0.23\n\n\n9\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.11\n0.11\n0.09\n0.00\n0.00\n0.00\n0.00\n0.05\n0.88\n0.98\n0.98\n0.67\n0.03\n\n\n10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\n0.95\n0.98\n0.98\n0.56\n0.00\n\n\n11\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.34\n0.74\n0.98\n0.98\n0.98\n0.05\n0.00\n\n\n12\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.36\n0.83\n0.96\n0.98\n0.98\n0.98\n0.80\n0.04\n0.00\n\n\n\n\n\n\nIf we compute the convolution over the \\(3\\times 3\\) window along rows \\(3\\) to \\(5\\) and columns \\(14\\) to \\(16\\) (the top edge of the \\(3\\)), the result will be:\n\n(im3[3:6,14:17] * top_edge).sum()\n\ntensor(2.9727)\n\n\nSimilarly, doing this in the window made by rows \\(7, 8, 9\\) and columns \\(14, 15, 16\\) (the bottom edge of \\(3\\)), we’ll get:\n\n(im3[7:10, 14:17] * top_edge).sum()\n\ntensor(-2.9570)\n\n\nThe positive pixels represent the “lighter” pixels and negative values represent the “darker” pixels in relative terms. Our kernel should be able to highlight the top edges of \\(3\\) by making the top edges lighter and the bottom edges darker.\nNote that to compute the convolutions, we are simply doing numpy’s element-wise multiplication followed by a sum.\nLet’s create a function which will compute the the convolutions over any \\(3 \\times 3\\) window with any \\(3 \\times 3\\) kernel.\n\n#row and col define the center coordinate of a 3x3 window\ndef apply_kernel(row, col, kernel): return (im3[row-1:row+2, col-1:col+2] * kernel).sum()\n\n\napply_kernel(4, 15, top_edge)\n\ntensor(2.9727)\n\n\nNext, we want to slide this kernel over the entire image. To do this, we’ll use list comprehension inside a list comprehension to create coordinates over which we want to move.\n\nrng = range(1, 27)\ntop_edge3 = tensor([[apply_kernel(i, j, top_edge) for j in rng] for i in rng])\nshow_image(top_edge3);\n\n\n\n\n\n\n\n\nOur simple little \\(3 \\times 3\\) kernel has managed to highlight the top edges of the digit \\(3\\) !\nHere’s a subtle thing that we need to notice.\n\ntop_edge3.shape\n\ntorch.Size([26, 26])\n\n\nThe shape of the output image is \\(26\\times 26\\) while the input image had \\(28\\times 28\\) dimensions. It is easy to see why the output dimensions change if we visualize the kernel sliding over all the \\(3\\times 3\\) windows in our input image. In general, if a kernel of dimension \\(f\\times f\\) convolve over an image of dimension \\(N\\times N\\), the output will have dimensions \\((N-f+1)\\times (N-f+1)\\).\nLet’s repeat the same exercise using another kernel which should be able to highlight the left edges.\n\nleft_edge = tensor([[-1, 0, 1],\n                    [-1, 0, 1],\n                    [-1, 0, 1]]).float()\nshow_image(left_edge, noframe=False);\n\n\n\n\n\n\n\n\n\nleft_edge3 = tensor([[apply_kernel(i, j, left_edge) for j in rng] for i in rng])\nshow_image(left_edge3);\n\n\n\n\n\n\n\n\nAnd it works as expected.\n\n\nConvolutions in PyTorch\nThe convolutions that we implemented in python above are quite slow.\n\n%timeit -n 1 tensor([[apply_kernel(i, j, left_edge) for j in rng] for i in rng])\n\n7.3 ms ± 1.53 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nPyTorch has an Conv2d class that is optimized for convolutions and offers added features and benefits.\n\ninp = im3[None,None,:,:].float()\n\nconv2d expects the input image to be a rank-four tensor where the dimensions correspond to batch, channel, height, width respectively. Since we are only considering a single image, our batch size (first dimension) is one, and we are working with black-and-white images, our images have a single channel. Colored images have three channels: red, green, and blue.\n\n%timeit -n 100 F.conv2d(inp, left_edge[None,None])\n\nThe slowest run took 44.46 times longer than the fastest. This could mean that an intermediate result is being cached.\n105 µs ± 219 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nConvolutions over a batch of images\nWhen training our network, we pass the input images to the models as batches. conv2d allows us to conveniently implement the convolutions using multiple kernels simultaneously over a batch of images.\nLet’s make two more kernels to detect diagnals in the image.\n\ndiag1_edge = tensor([[ 0,-1, 1],\n                     [-1, 1, 0],\n                     [ 1, 0, 0]]).float()\n\n\nshow_image(diag1_edge, noframe=False);\n\n\n\n\n\n\n\n\n\ndiag2_edge = tensor([[ 1,-1, 0],\n                     [ 0, 1,-1],\n                     [ 0, 0, 1]]).float()\n\n\nshow_image(diag2_edge);\n\n\n\n\n\n\n\n\nLet’s create a batch of 16 images.\n\nxb = x_imgs[:16][:, None]\nxb.shape\n\ntorch.Size([16, 1, 28, 28])\n\n\nAnd stack all our kernels together\n\nedge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])[:, None]\nedge_kernels.shape\n\ntorch.Size([4, 1, 3, 3])\n\n\nNow, we pass the batch and kernel to conv2d\n\nbatch_features = F.conv2d(xb, edge_kernels)\nbatch_features.shape\n\ntorch.Size([16, 4, 26, 26])\n\n\nThe output indicates that we have \\(16\\) images in the batch, \\(4\\) filters, and each image is of dimension \\(26\\times26\\).\nLet’s see what the kernels detect when applied on a particular image\n\nx = xb[1, 0]\nshow_image(x);\n\n\n\n\n\n\n\n\n\nshow_images([batch_features[1,i] for i in range(4)]);\n\n\n\n\n\n\n\n\n\n\nStrides and Padding\nUsing the convolution operation, we lose some pixels from our image, effectively losing data, which is never good. Using appropriate padding, we can ensure that the output activation map is the same size as the input image.  &gt;Figure 4: Padding\nZero padding is a common approach where the input outside the valid range is \\(0\\). If we pad the images so that the output dimensions are the same as the input dimensions, then it’s called valid padding. If we add a kernel of size \\(f\\times f\\) (with \\(f\\) an odd number), the necessary padding on each side to keep the same shape is \\(f//2\\).\nSo far, we’ve seen our kernel slide over the image grid by one unit, or in other words, it has a stride of 1. However, a kernel can have a stride greater than \\(1\\) as well. A stride of 2 means creating roughly half the number of outputs.\n &gt; Figure 5: stride-2 convolutions\nIf we have an image of size \\(n\\times n\\), a kernel of size \\(f\\times f\\), padding of \\(p\\) pixels on each side, and a stride of \\(s\\) then the output of the convolution operation will have a size of \\((\\frac{n-f+2p}{s} + 1) \\times (\\frac{n-f+2p}{s} + 1)\\).\n &gt; Figure 6: With a \\(5\\times 5\\) input, \\(4\\times 4\\) kernel, and \\(2\\) pixels of padding, we end up with a \\(6\\times 6\\) activation map.\n\n\nCreating a CNN\nWe are now able to create and train a Convolutional Neural Network. But before that, let’s recall how we’d build a simple one-layer MLP using nn.Sequential.\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\n\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\nWhat if we use the same idea to build a CNN as well.\n\nbroken_cnn = nn.Sequential(\n    nn.Conv2d(1, 30, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(30, 10, kernel_size=3, padding=1)\n)\n\n\nbroken_cnn(xb).shape\n\ntorch.Size([16, 10, 28, 28])\n\n\nWe want to have 10 output channels for each of the 16 images in the batch which isn’t the case here.\nTo make our ConvNet architecture, we’ll first create a conv function with appropriate input channels, output channels, stride, kernel size, and padding which returns a sequential model with an optional activation function.\n\ndef conv(ni, nf, ks=3, stride=2, act=True):\n    res = nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\nand then put those conv functions together to get our ConvNet architecture. The nn.Flatten() removes the unneccesary unit axes. The commented dimensions represent the dimensions of the output from that conv layer.\n\nsimple_cnn = nn.Sequential(\n    conv(1, 4),                 #14x14\n    conv(4, 8),                 #7x7\n    conv(8, 16),                #4x4\n    conv(16, 16),               #2x2\n    conv(16, 10, act=False),    #1x1\n    nn.Flatten()\n)\n\n\nsimple_cnn(xb).shape\n\ntorch.Size([16, 10])\n\n\nNow lets create our Datasets and DataLoaders to create batches for training.\n\nx_imgs = x_train.view(-1, 1, 28, 28)\nxv_imgs = x_valid.view(-1, 1, 28, 28)\ntrain_ds, valid_ds = Dataset(x_imgs, y_train), Dataset(xv_imgs, y_valid)\n\n\ndef_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef to_device(x, device=def_device):\n    if isinstance(x, torch.Tensor): return x.to(device)\n    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n    return type(x)(to_device(o, device) for o in x)\n\ndef collate_device(b): return to_device(default_collate(b))\n\n\nbs = 256\nlr = 0.4\ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, bs, collate_fn=collate_device)\nopt = optim.SGD(simple_cnn.parameters(), lr=lr)\n\nNotice that we move our model (and its parameters) to the gpu (if its available) to speed up training.\nWe’ll be using accuracy as a metric.\nLet’s call fit and train for 5 epochs.\n\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\nepoch:0, loss:0.5821727288246155, accuracy:0.807\nepoch:1, loss:0.16650847618877887, accuracy:0.9497\nepoch:2, loss:0.2013450484395027, accuracy:0.9395\nepoch:3, loss:0.10804087935984134, accuracy:0.9689\nepoch:4, loss:0.10930393682271242, accuracy:0.9707\n\n\nand it seems to be working fine.\nNow let’s reduce the learning rate to 0.1 and train again for 5 epochs.\n\nopt = optim.SGD(simple_cnn.parameters(), lr=lr/4)\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\nepoch:0, loss:0.08476311807408929, accuracy:0.9755\nepoch:1, loss:0.08613430197462439, accuracy:0.9758\nepoch:2, loss:0.08058924336060881, accuracy:0.9777\nepoch:3, loss:0.08108808109462261, accuracy:0.978\nepoch:4, loss:0.07943615848198533, accuracy:0.9782\n\n\n… and accuracy has improved to 0.97!\nThat was all about building and training ConvNets from the ground up.\nThankyou for reading."
  },
  {
    "objectID": "blog/posts/ConvNets/index.html",
    "href": "blog/posts/ConvNets/index.html",
    "title": "Convolutional Neural Networks from Scratch",
    "section": "",
    "text": "This blog post is a code-first introduction to Convolutional Networks (ConvNets). We will build our own ConvNet from scratch and understand the fundamentals of how and why they work so well, particularly for image processing tasks.\nNB: This blog uses some functions from the little_ai library I am currently building.\n\nImporting Libraries\n\nimport torch\nfrom torch import nn\n\nfrom torch.utils.data import default_collate\nfrom typing import Mapping\n\nfrom little_ai.training import *\nfrom little_ai.datasets import *\n\n\nimport pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\nimport pandas as pd,matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\nfrom torch import tensor\nimport torch.nn.functional as F\nfrom torch import optim\n\nfrom torch.utils.data import DataLoader\nfrom typing import Mapping\n\n\nmpl.rcParams['image.cmap'] = 'gray'\n\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n\n\n\nWhy ConvNets\n &gt; Figure 1: A simple neural net with two hidden layers\nImages have properties that necessitate the need for specialized architectures.\n\nThey are high-dimensional. RGB images of dimensions \\(224\\times224\\) are very common (i.e., \\(150, 528\\) input dimensions). Hidden layers in fully connected networks are generally larger than the input size, so even for a shallow network, the number of weights would exceed \\(150,528^2\\) or \\(22\\) billion. This poses obvious practical problems in terms of the required training data, memory, and computation.\nNearby image pixels are statistically related. However, fully connected networks have no notion of “nearby” and treat the relation between every input equally.\nThe interpretation of an image is stable under geometric transformations. An image of a tree is still an image of a tree if we shift it leftwards by a few pixels. However, this shift is stable under geometric transformations. Hence, a fully connected model must learn the patterns of pixels that signify a tree separately at every position, which is inefficient.\n\nConvolutional layers process each local image region independently, using parameters shared across the whole image. They use fewer parameters than fully connected layers, exploit the spatial relationships between nearby pixels, and don’t have to re-learn the interpretation of pixels at every position.\n &gt; Figure 2: Architecture of a simple ConvNet\n\n\nConvolution Operation\nConvolutional layers perform the convolution operation using kernels (or filters) which is simply a matrix, over an image. Convolution is a simple mathematical operation that involves addition and multiplication.\n\n\n\nConvolution Operation\n\n\n\nFigure 3: Convolution operation using a \\(2\\times2\\) kernel over a \\(3\\times4\\) image that results in a \\(2\\times3\\) output matrix. The kernel slides over the image, adding the products of the overlapping matrix values. Source: Deep Learning by Goodfellow, et al.\n\nNow, let’s implement our own convolution operation using numpy. We’ll choose an image from the training set and create a simple \\(3\\times 3\\) matrix as our kernel and, as we’ll see, the convolution operation with the right choice of the kernel will be able to identify some pattern in our image.\n\nx_imgs = x_train.view(-1, 28, 28)\nxv_imgs = x_valid.view(-1, 28, 28)\n\n\nmpl.rcParams['figure.dpi'] = 30\n\n\nim3 = x_imgs[7]\nshow_image(im3);\n\n\n\n\n\n\n\n\nLet’s create a simple \\(3\\times3\\) kernel (people in the computer vision world call it a kernel; essentially it’s just a tensor).\nNB: Actually, the values of kernels are not entered manually but are learned as parameters during training.\n\ntop_edge = tensor([[-1, -1, -1], \n                  [0, 0, 0], \n                  [1, 1, 1]]).float()\n\n\nshow_image(top_edge, noframe=False);\n\n\n\n\n\n\n\n\nThis kernel will slide along all the \\(3\\times 3\\) windows in our images and compute the convolutions. If \\(3\\times 3\\)` window looks like this:\n\\[\\begin{matrix} a & b & c \\\\ d & e & f \\\\ g & h & i\\end{matrix}\\]\nthen the result will be \\(-a -b -c +g+h+i\\)\n\ndf = pd.DataFrame(im3[:13,:23])\ndf.style.format(precision=2).set_properties(**{'font-size':'7pt'}).background_gradient('Greys')\n\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n\n\n\n\n0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n2\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n3\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n4\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n5\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.15\n0.17\n0.41\n1.00\n0.99\n0.99\n0.99\n0.99\n0.99\n0.68\n0.02\n0.00\n\n\n6\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.17\n0.54\n0.88\n0.88\n0.98\n0.99\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.62\n0.05\n\n\n7\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.70\n0.98\n0.98\n0.98\n0.98\n0.99\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.23\n\n\n8\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.43\n0.98\n0.98\n0.90\n0.52\n0.52\n0.52\n0.52\n0.74\n0.98\n0.98\n0.98\n0.98\n0.23\n\n\n9\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.02\n0.11\n0.11\n0.09\n0.00\n0.00\n0.00\n0.00\n0.05\n0.88\n0.98\n0.98\n0.67\n0.03\n\n\n10\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.33\n0.95\n0.98\n0.98\n0.56\n0.00\n\n\n11\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.34\n0.74\n0.98\n0.98\n0.98\n0.05\n0.00\n\n\n12\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.36\n0.83\n0.96\n0.98\n0.98\n0.98\n0.80\n0.04\n0.00\n\n\n\n\n\n\nIf we compute the convolution over the \\(3\\times 3\\) window along rows \\(3\\) to \\(5\\) and columns \\(14\\) to \\(16\\) (the top edge of the \\(3\\)), the result will be:\n\n(im3[3:6,14:17] * top_edge).sum()\n\ntensor(2.9727)\n\n\nSimilarly, doing this in the window made by rows \\(7, 8, 9\\) and columns \\(14, 15, 16\\) (the bottom edge of \\(3\\)), we’ll get:\n\n(im3[7:10, 14:17] * top_edge).sum()\n\ntensor(-2.9570)\n\n\nThe positive pixels represent the “lighter” pixels and negative values represent the “darker” pixels in relative terms. Our kernel should be able to highlight the top edges of \\(3\\) by making the top edges lighter and the bottom edges darker.\nNote that to compute the convolutions, we are simply doing numpy’s element-wise multiplication followed by a sum.\nLet’s create a function which will compute the the convolutions over any \\(3 \\times 3\\) window with any \\(3 \\times 3\\) kernel.\n\n#row and col define the center coordinate of a 3x3 window\ndef apply_kernel(row, col, kernel): return (im3[row-1:row+2, col-1:col+2] * kernel).sum()\n\n\napply_kernel(4, 15, top_edge)\n\ntensor(2.9727)\n\n\nNext, we want to slide this kernel over the entire image. To do this, we’ll use list comprehension inside a list comprehension to create coordinates over which we want to move.\n\nrng = range(1, 27)\ntop_edge3 = tensor([[apply_kernel(i, j, top_edge) for j in rng] for i in rng])\nshow_image(top_edge3);\n\n\n\n\n\n\n\n\nOur simple little \\(3 \\times 3\\) kernel has managed to highlight the top edges of the digit \\(3\\) !\nHere’s a subtle thing that we need to notice.\n\ntop_edge3.shape\n\ntorch.Size([26, 26])\n\n\nThe shape of the output image is \\(26\\times 26\\) while the input image had \\(28\\times 28\\) dimensions. It is easy to see why the output dimensions change if we visualize the kernel sliding over all the \\(3\\times 3\\) windows in our input image. In general, if a kernel of dimension \\(f\\times f\\) convolve over an image of dimension \\(N\\times N\\), the output will have dimensions \\((N-f+1)\\times (N-f+1)\\).\nLet’s repeat the same exercise using another kernel which should be able to highlight the left edges.\n\nleft_edge = tensor([[-1, 0, 1],\n                    [-1, 0, 1],\n                    [-1, 0, 1]]).float()\nshow_image(left_edge, noframe=False);\n\n\n\n\n\n\n\n\n\nleft_edge3 = tensor([[apply_kernel(i, j, left_edge) for j in rng] for i in rng])\nshow_image(left_edge3);\n\n\n\n\n\n\n\n\nAnd it works as expected.\n\n\nConvolutions in PyTorch\nThe convolutions that we implemented in python above are quite slow.\n\n%timeit -n 1 tensor([[apply_kernel(i, j, left_edge) for j in rng] for i in rng])\n\n7.82 ms ± 1.23 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nPyTorch has an Conv2d class that is optimized for convolutions and offers added features and benefits.\n\ninp = im3[None,None,:,:].float()\n\nconv2d expects the input image to be a rank-four tensor where the dimensions correspond to batch, channel, height, width respectively. Since we are only considering a single image, our batch size (first dimension) is one, and we are working with black-and-white images, our images have a single channel. Colored images have three channels: red, green, and blue.\n\n%timeit -n 100 F.conv2d(inp, left_edge[None,None])\n\n23 µs ± 10.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nConvolutions over a batch of images\nWhen training our network, we pass the input images to the models as batches. conv2d allows us to conveniently implement the convolutions using multiple kernels simultaneously over a batch of images.\nLet’s make two more kernels to detect diagnals in the image.\n\ndiag1_edge = tensor([[ 0,-1, 1],\n                     [-1, 1, 0],\n                     [ 1, 0, 0]]).float()\n\n\nshow_image(diag1_edge, noframe=False);\n\n\n\n\n\n\n\n\n\ndiag2_edge = tensor([[ 1,-1, 0],\n                     [ 0, 1,-1],\n                     [ 0, 0, 1]]).float()\n\n\nshow_image(diag2_edge);\n\n\n\n\n\n\n\n\nLet’s create a batch of 16 images.\n\nxb = x_imgs[:16][:, None]\nxb.shape\n\ntorch.Size([16, 1, 28, 28])\n\n\nAnd stack all our kernels together\n\nedge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])[:, None]\nedge_kernels.shape\n\ntorch.Size([4, 1, 3, 3])\n\n\nNow, we pass the batch and kernel to conv2d\n\nbatch_features = F.conv2d(xb, edge_kernels)\nbatch_features.shape\n\ntorch.Size([16, 4, 26, 26])\n\n\nThe output indicates that we have \\(16\\) images in the batch, \\(4\\) filters, and each image is of dimension \\(26\\times26\\).\nLet’s see what the kernels detect when applied on a particular image\n\nx = xb[1, 0]\nshow_image(x);\n\n\n\n\n\n\n\n\n\nshow_images([batch_features[1,i] for i in range(4)]);\n\n\n\n\n\n\n\n\n\n\nStrides and Padding\nUsing the convolution operation, we lose some pixels from our image, effectively losing data, which is never good. Using appropriate padding, we can ensure that the output activation map is the same size as the input image.  &gt;Figure 4: Padding\nZero padding is a common approach where the input outside the valid range is \\(0\\). If we pad the images so that the output dimensions are the same as the input dimensions, then it’s called valid padding. If we add a kernel of size \\(f\\times f\\) (with \\(f\\) an odd number), the necessary padding on each side to keep the same shape is \\(f//2\\).\nSo far, we’ve seen our kernel slide over the image grid by one unit, or in other words, it has a stride of 1. However, a kernel can have a stride greater than \\(1\\) as well. A stride of 2 means creating roughly half the number of outputs.\n &gt; Figure 5: stride-2 convolutions\nIf we have an image of size \\(n\\times n\\), a kernel of size \\(f\\times f\\), padding of \\(p\\) pixels on each side, and a stride of \\(s\\) then the output of the convolution operation will have a size of \\((\\frac{n-f+2p}{s} + 1) \\times (\\frac{n-f+2p}{s} + 1)\\).\n &gt; Figure 6: With a \\(5\\times 5\\) input, \\(4\\times 4\\) kernel, and \\(2\\) pixels of padding, we end up with a \\(6\\times 6\\) activation map.\n\n\nCreating a CNN\nWe are now able to create and train a Convolutional Neural Network. But before that, let’s recall how we’d build a simple one-layer MLP using nn.Sequential.\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\n\n\nmodel = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n\nWhat if we use the same idea to build a CNN as well.\n\nbroken_cnn = nn.Sequential(\n    nn.Conv2d(1, 30, kernel_size=3, padding=1),\n    nn.ReLU(),\n    nn.Conv2d(30, 10, kernel_size=3, padding=1)\n)\n\n\nbroken_cnn(xb).shape\n\ntorch.Size([16, 10, 28, 28])\n\n\nWe want to have 10 output channels for each of the 16 images in the batch which isn’t the case here.\nTo make our ConvNet architecture, we’ll first create a conv function with appropriate input channels, output channels, stride, kernel size, and padding which returns a sequential model with an optional activation function.\n\ndef conv(ni, nf, ks=3, stride=2, act=True):\n    res = nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2)\n    if act: res = nn.Sequential(res, nn.ReLU())\n    return res\n\nand then put those conv functions together to get our ConvNet architecture. The nn.Flatten() removes the unneccesary unit axes. The commented dimensions represent the dimensions of the output from that conv layer.\n\nsimple_cnn = nn.Sequential(\n    conv(1, 4),                 #14x14\n    conv(4, 8),                 #7x7\n    conv(8, 16),                #4x4\n    conv(16, 16),               #2x2\n    conv(16, 10, act=False),    #1x1\n    nn.Flatten()\n)\n\n\nsimple_cnn(xb).shape\n\ntorch.Size([16, 10])\n\n\nNow lets create our Datasets and DataLoaders to create batches for training.\n\nx_imgs = x_train.view(-1, 1, 28, 28)\nxv_imgs = x_valid.view(-1, 1, 28, 28)\ntrain_ds, valid_ds = Dataset(x_imgs, y_train), Dataset(xv_imgs, y_valid)\n\n\ndef_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef to_device(x, device=def_device):\n    if isinstance(x, torch.Tensor): return x.to(device)\n    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n    return type(x)(to_device(o, device) for o in x)\n\ndef collate_device(b): return to_device(default_collate(b))\n\n\nbs = 256\nlr = 0.4\ntrain_dl,valid_dl = get_dls(train_ds, valid_ds, bs, collate_fn=collate_device)\nopt = optim.SGD(simple_cnn.parameters(), lr=lr)\n\nNotice that we move our model (and its parameters) to the gpu (if its available) to speed up training.\nWe’ll be using accuracy as a metric.\nLet’s call fit and train for 5 epochs.\n\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\nepoch:0, loss:0.3939493108987808, accuracy:0.8802\nepoch:1, loss:0.1568004638493061, accuracy:0.9541\nepoch:2, loss:0.14168763396739958, accuracy:0.9604\nepoch:3, loss:0.1379448724284768, accuracy:0.9602\nepoch:4, loss:0.12180517784804105, accuracy:0.9635\n\n\nand it seems to be working fine.\nNow let’s reduce the learning rate to 0.1 and train again for 5 epochs.\n\nopt = optim.SGD(simple_cnn.parameters(), lr=lr/4)\nloss,acc = fit(5, simple_cnn.to(def_device), F.cross_entropy, opt, train_dl, valid_dl)\n\nepoch:0, loss:0.08432787099350243, accuracy:0.9761\nepoch:1, loss:0.08465766663141548, accuracy:0.9762\nepoch:2, loss:0.08055750963222236, accuracy:0.976\nepoch:3, loss:0.08232340762391686, accuracy:0.9753\nepoch:4, loss:0.08031454239357262, accuracy:0.976\n\n\n… and accuracy has improved to 0.97!\nThat was all about building and training ConvNets from the ground up.\nThankyou for reading."
  }
]