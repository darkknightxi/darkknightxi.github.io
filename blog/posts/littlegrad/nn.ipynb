{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "GRi-3uwRDqc5"
   },
   "source": [
    "---\n",
    "title: \"Building littlegrad Part 2\"\n",
    "author: \"Kartikeya Khare\"\n",
    "date: \"April 25, 2024\"\n",
    "categories: [Deep Learning]\n",
    "image: \"littlegrad2.jpg\"\n",
    "format:\n",
    "    html:\n",
    "        toc: true\n",
    "        toc-title: Contents\n",
    "        toc-location: left\n",
    "execute:\n",
    "    enabled: false\n",
    "    output: true\n",
    "    cache: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Part 1](https://darkknightxi.github.io/blog/posts/littlegrad/littlegrad.html) we created a full-fledged autograd engine from the ground up. In this part, we will build and train our own little neural network by utilising the autograd engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoCVM7oND_q9"
   },
   "source": [
    "# Building a Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUCMBfyREKhd"
   },
   "source": [
    "An MLP is a neural network containing one or more hidden layers. A single neuron in an MLP computes and outputs the activations (linear relation followed by an (optional) non-linear function) of its inputs.\n",
    "\n",
    "<img src=\"nn.png\">\n",
    "\n",
    "If $W$ is the weight matrix and $X$ is the input matrix, the output of a single neuron is,\n",
    "\n",
    "$$a = f(W\\cdot X + b)$$\n",
    ", where $f$ is a non-linear activation function and $b$ is the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9psDU5COT-Z"
   },
   "source": [
    "Now, lets implement a `Neuron` class that does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ldgQZECmOaVR"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, nin):\n",
    "    self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "    self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    act = sum(wi*xi for wi, xi in zip(self.w, x)) + self.b\n",
    "    return act.tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVdfT3-VgAmo"
   },
   "source": [
    "The `__call__` method implements the forward pass of the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk0KHH7zfzyi"
   },
   "source": [
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1eDWd550f10N",
    "outputId": "f06e3c9d-3882-403c-bb53-6db96fc68cca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.18630956619533254)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Neuron(2) # a neuron having two inputs\n",
    "x = [0.5, -2.]\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsrtLr6j_SBD"
   },
   "source": [
    "Here's whats happening: The neuron has 2 inputs, `x1` and `x2` which are multiplied with weights `w1` and `w2`, which are sampled uniformly between `-1` and `1`, and the bias term `b` is added to the result, `w1*x1 + w2*x2 + b` and this linear representation is passed through the `tanh` function to give the neuron's output,\n",
    "$$\n",
    "\\tanh(w_1\\cdot x_1 + w_2\\cdot x_2 + b)\n",
    "$$\n",
    "Pretty straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCEnL7rPKTrC"
   },
   "source": [
    "Next up, let's implement `Layer` which is just a bunch of neurons which aren't connected to each other but are fully connected to the input (the previous layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ShA3D5zQKfOu"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  def __init__(self, nin, nout):\n",
    "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    outs = [n(x) for n in self.neurons]\n",
    "    return outs[0] if len(outs) == 1 else outs # if there's a single output, we don't want it to be wrapped in a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZObBJx28LmTr"
   },
   "source": [
    "Again, the code should be quite self explanatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOMdlv42LsSR"
   },
   "source": [
    "Let's see a layer in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4t8AN9YmLwQt",
    "outputId": "25186769-7d42-4220-a58c-8c3c4d24e621"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9316061471543342),\n",
       " Value(data=-0.9925428606439287),\n",
       " Value(data=0.7940286717352634)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [0.5, -2.]\n",
    "l = Layer(2, 3) #2 inputs, 3 independent neurons\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vCV11WbMQm2"
   },
   "source": [
    "3 outputs as expected. So far so good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nad9QFlIMUYa"
   },
   "source": [
    "Now, lets pull up all our neurons and layers together to create an MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGVvBZh7O2l5"
   },
   "source": [
    "`nouts` is a list of neurons in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "_0gfOaUjMbRU"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "  def __init__(self, nin, nouts):\n",
    "    sz = [nin] + nouts\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2GDwxdXO_tB"
   },
   "source": [
    "Lets run a forward pass of this MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QAf_kDmPGDJ",
    "outputId": "74870e70-c0b0-4398-9c4b-85ea3b61ee96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.8973431783444543)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# three inputs\n",
    "x = [1, 2, 3]\n",
    "\n",
    "#3 layers, first having 4 neurons, the second having 2, and the last having a single neuron\n",
    "nouts = [4, 2, 1]\n",
    "\n",
    "#model\n",
    "m = MLP(3, nouts)\n",
    "\n",
    "#forward pass\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1OweBIKQMQJ"
   },
   "source": [
    "Neat! We've got out little MLP running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-y6HRpRXSNUO"
   },
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIfEa_wsSVN5"
   },
   "source": [
    "We've build a simple neural network, that's all great but how do we know if its any good? This is where loss functions come into the picture. Loss function gives us a single scalar value that allows us to quantitatively judge a model's performance. Lower the loss, the better is the model. Thus, *training* a neural network refers to choosing the right set of parameters that lead to a lower loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GR92ef4TUpG"
   },
   "source": [
    "We'll begin by creating a tiny dataset. The `Y` values correspond to the output for the corresponding input `X`. We essentially want our neural network to perform binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Bsqk81ZIToeY"
   },
   "outputs": [],
   "source": [
    "X = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "Y = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huHGhoyrUBs1"
   },
   "source": [
    "Let's see what our MLP with default parameters predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QNGsaV8FUKKt",
    "outputId": "bb2deb01-4d7f-4465-b057-a35e5cd29972"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.9731970202739209),\n",
       " Value(data=-0.8965799622430423),\n",
       " Value(data=-0.9641324434027786),\n",
       " Value(data=-0.9215067677945139)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [m(x) for x in X]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqpDFgyAVH6P"
   },
   "source": [
    "The MLP is performing pretty terribly at the moment. How bad you ask? Let's use a simple squared error loss function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BqAasFaMVZSN",
    "outputId": "bb0b065b-e5c6-41dc-db16-263a9467aa0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=7.597676925323905)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = sum(((y_pred-y)**2 for y_pred, y in zip(preds, Y)))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDqhOnIpXGs9"
   },
   "source": [
    "The loss is `0` when our predictions are exactly equal to the true values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqUaT-i4XnyC"
   },
   "source": [
    "But how do we minimize the loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrXZxtyZ5S8V"
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v2hQV3yXqq2"
   },
   "source": [
    "Let's call the `.backward()` method on the `loss` and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "T890XqPxXzfi"
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2LqTAcrX2kb",
    "outputId": "60552de5-a3f1-4462-fe5f-9d3fbe1fdd93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3774265300484028"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.layers[0].neurons[0].w[0].grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYOFt0oyYBMT"
   },
   "source": [
    "The weights of the neurons now have (non-zero) gradients thanks to out backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1osjvfVFYNdH"
   },
   "source": [
    "The negative gradient indicates that increasing this particular weight will make the loss go down. Similarly if we nudge all the parameters (weights and biases) in the direction opposite to their gradient, we can expect to minimize the loss. To do this, let's first collect all model parameters so that we can update them all in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "GvIRCj0qX11D"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, nin):\n",
    "    self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "    self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    act = sum(wi*xi for wi, xi in zip(self.w, x)) + self.b\n",
    "    return act.tanh()\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "OR_xFnNuX1E9"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  def __init__(self, nin, nout):\n",
    "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    outs = [n(x) for n in self.neurons]\n",
    "    return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "  def parameters(self):\n",
    "    params = []\n",
    "    for n in self.neurons:\n",
    "      ps = n.parameters()\n",
    "      params.extend(ps)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "fjQH0afRe99S"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "  def __init__(self, nin, nouts):\n",
    "    sz = [nin] + nouts\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "  def parameters(self):\n",
    "    params = []\n",
    "    for l in self.layers:\n",
    "      ps = l.parameters()\n",
    "      params.extend(ps)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW8v4EFxfq80"
   },
   "source": [
    "Now, we can grab all the parameters of the network like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxAmg0o-fyFO",
    "outputId": "d7a08883-c148-4b20-87e4-fcc301d739be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9570846787972174),\n",
       " Value(data=0.31915267955407445),\n",
       " Value(data=0.7018955388242962),\n",
       " Value(data=-0.3335361821232903),\n",
       " Value(data=-0.599362755237959),\n",
       " Value(data=-0.3458541523874765),\n",
       " Value(data=0.6934652303346629),\n",
       " Value(data=-0.5557157651208695),\n",
       " Value(data=-0.7497140978164074),\n",
       " Value(data=-0.8952754566500449),\n",
       " Value(data=-0.938458042336328),\n",
       " Value(data=-0.028495997807620466),\n",
       " Value(data=-0.06475822748304472),\n",
       " Value(data=0.40681450338062586),\n",
       " Value(data=-0.40169697478775035),\n",
       " Value(data=0.30806358721514426),\n",
       " Value(data=-0.7340709666658356),\n",
       " Value(data=0.18919788644542823),\n",
       " Value(data=-0.5804923344061348),\n",
       " Value(data=-0.22274505607706163),\n",
       " Value(data=0.9938108289241792),\n",
       " Value(data=0.6214299384656257),\n",
       " Value(data=0.4912326104078326),\n",
       " Value(data=0.033810534355125155),\n",
       " Value(data=0.8849979216108752),\n",
       " Value(data=-0.6443632355448854),\n",
       " Value(data=-0.31275994137978014),\n",
       " Value(data=0.2997596033736216),\n",
       " Value(data=-0.804460736802685)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3]\n",
    "nouts = [4, 2, 1]\n",
    "m = MLP(3, nouts)\n",
    "m(x)\n",
    "m.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "purzy4KLjD-b"
   },
   "source": [
    "Now that we have all our parameters in one place, we will update their values based on the gradient information. Nudging the parameter value in the opposite direction to the gradient minimizes the loss.\n",
    "\n",
    "<img src = \"gradient_descent.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "O8pFVz61fxb7"
   },
   "outputs": [],
   "source": [
    "for p in m.parameters():\n",
    "  p.data -= 0.01*p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L5LGvKHnlkN"
   },
   "source": [
    "The scalar factor ($0.01$ in this case) determines how big of a step we are taking in the direction of the minima. It is called the learning rate and is a hyperparameter of the model (that is, it is not learned during training but set manually). The higher the learning rate, the faster we'll approach the minima, but with a too high value, there's a chance of overshooting and the loss increasing instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODIY6LV9oNTl"
   },
   "source": [
    "Now let's again use our model to make predictions and evaluate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ilnKHE8oZZ4",
    "outputId": "c9cacb5d-85b1-412e-e5bd-3717e5134695"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=6.208941217251937)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [m(x) for x in X]\n",
    "loss = sum(((y_pred-y)**2 for y_pred, y in zip(preds, Y)))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAKUeCERow0V"
   },
   "source": [
    "Okay, so our loss has gone down a little. If we keep on updating the parameters, the loss will (hopefully) keep getting lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siF4d71DzZ0d"
   },
   "source": [
    "Finally, we can write down a training loop with the forward pass, backward pass and parameter update. We'll run this loop 20 times (called the number of epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOl3R643fxH8",
    "outputId": "7b02ae0d-1ef8-42f7-b645-1b84f3887541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 6.208941217251937\n",
      "epoch: 1, loss: 6.056959827747116\n",
      "epoch: 2, loss: 5.723776432733178\n",
      "epoch: 3, loss: 5.160490416926962\n",
      "epoch: 4, loss: 4.3521441493058415\n",
      "epoch: 5, loss: 3.4703651333760837\n",
      "epoch: 6, loss: 2.962443134261985\n",
      "epoch: 7, loss: 3.0696934813703454\n",
      "epoch: 8, loss: 3.3995978230827695\n",
      "epoch: 9, loss: 3.4821204012479656\n",
      "epoch: 10, loss: 3.083212945879714\n",
      "epoch: 11, loss: 1.9966522842914274\n",
      "epoch: 12, loss: 0.9232860127290902\n",
      "epoch: 13, loss: 0.4478367575015849\n",
      "epoch: 14, loss: 0.26532143242872064\n",
      "epoch: 15, loss: 0.650759735046911\n",
      "epoch: 16, loss: 0.2510464892017052\n",
      "epoch: 17, loss: 0.03805675377993294\n",
      "epoch: 18, loss: 0.014157092884907388\n",
      "epoch: 19, loss: 0.006543777589572971\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "\n",
    "  #forward pass\n",
    "  preds = [m(x) for x in X]\n",
    "  loss = sum(((y_pred-y)**2 for y_pred, y in zip(preds, Y)))\n",
    "\n",
    "  #backward pass\n",
    "  loss.backward()\n",
    "\n",
    "  #update params\n",
    "  for p in m.parameters():\n",
    "    p.data -= 0.01 * p.grad\n",
    "\n",
    "  print(f'epoch: {k}, loss: {loss.data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyyjvXB-1zB7"
   },
   "source": [
    "And there you go! We've successfully managed to build and train a simple MLP from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYyvfN6P2E1L"
   },
   "source": [
    "Let's see how the predictions look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ZEozBi62Ies",
    "outputId": "1c7d6d8a-a01a-4824-fed0-a1bfb1c52b26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9866460176051887),\n",
       " Value(data=-0.9621584068227153),\n",
       " Value(data=-0.9617506297921213),\n",
       " Value(data=0.98876399399386)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [m(x) for x in X]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T_llGlC2Zcr"
   },
   "source": [
    "The predictions are now pretty close to the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJHKELN-2pHu"
   },
   "source": [
    "That's it. That was our MLP built and trained from first-principles. Here's the final consolidated code for the neural network with a few tweaks which should be self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "c3KB8mwR15CH"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "  def __init__(self, n_in, non_lin=True):\n",
    "    self.w = [Value(random.uniform(-1, 1)) for _ in range(n_in)]\n",
    "    self.b = Value(random.uniform(-1, 1))\n",
    "    self.non_lin = non_lin\n",
    "\n",
    "  def __call__(self, x):\n",
    "    act = sum(wi*xi for wi, xi in zip(self.w, x)) + self.b\n",
    "    return act.relu() if self.non_lin else act\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.w + [self.b]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"{'ReLU' if self.non_lin else 'Linear'}Neuron({len(self.w)})\"\n",
    "\n",
    "class Layer:\n",
    "\n",
    "  def __init__(self, n_in, n_out, **kwargs):\n",
    "    self.neurons = [Neuron(n_in, **kwargs) for _ in range(n_out)]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    outs = [n(x) for n in self.neurons]\n",
    "    return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "  def parameters(self):\n",
    "    return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "class MLP:\n",
    "\n",
    "  def __init__(self, n_in, n_outs):\n",
    "    sz = [n_in] + n_outs\n",
    "    self.layers = [Layer(sz[i], sz[i+1], non_lin=i!=len(n_outs)-1) for i in range(len(n_outs))]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for l in self.layers:\n",
    "      x = l(x)\n",
    "    return x\n",
    "\n",
    "  def parameters(self):\n",
    "    return [p for l in self.layers for p in l.parameters()]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
